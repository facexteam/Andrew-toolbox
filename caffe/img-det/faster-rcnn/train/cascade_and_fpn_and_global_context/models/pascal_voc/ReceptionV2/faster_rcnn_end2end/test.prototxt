name: "Inception-Resnet-v2"
### input ####
input: "data"
input_shape {
	dim: 1
	dim: 3
	dim: 299
	dim: 299
}
input: "im_info"
input_shape {
	dim: 1
	dim: 3
}
layer {
  name: "conv1_3x3_s2"
  type: "Convolution"
  bottom: "data"
  top: "conv1_3x3_s2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "conv1_3x3_s2_bn"
  type: "BatchNorm"
  bottom: "conv1_3x3_s2"
  top: "conv1_3x3_s2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv1_3x3_s2_scale"
  type: "Scale"
  bottom: "conv1_3x3_s2"
  top: "conv1_3x3_s2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv1_3x3_s2_relu"
  type: "ReLU"
  bottom: "conv1_3x3_s2"
  top: "conv1_3x3_s2"
}
layer {
  name: "conv2a_3x3_s1"
  type: "Convolution"
  bottom: "conv1_3x3_s2"
  top: "conv2a_3x3_s1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "conv2a_3x3_s1_bn"
  type: "BatchNorm"
  bottom: "conv2a_3x3_s1"
  top: "conv2a_3x3_s1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv2a_3x3_s1_scale"
  type: "Scale"
  bottom: "conv2a_3x3_s1"
  top: "conv2a_3x3_s1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2a_3x3_s1_relu"
  type: "ReLU"
  bottom: "conv2a_3x3_s1"
  top: "conv2a_3x3_s1"
}
layer {
  name: "conv2b_3x3_s1"
  type: "Convolution"
  bottom: "conv2a_3x3_s1"
  top: "conv2b_3x3_s1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "conv2b_3x3_s1_bn"
  type: "BatchNorm"
  bottom: "conv2b_3x3_s1"
  top: "conv2b_3x3_s1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv2b_3x3_s1_scale"
  type: "Scale"
  bottom: "conv2b_3x3_s1"
  top: "conv2b_3x3_s1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2b_3x3_s1_relu"
  type: "ReLU"
  bottom: "conv2b_3x3_s1"
  top: "conv2b_3x3_s1"
}
layer {
  name: "pool_3a"
  type: "Pooling"
  bottom: "conv2b_3x3_s1"
  top: "pool_3a"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3_1x1_s1"
  type: "Convolution"
  bottom: "pool_3a"
  top: "conv3_1x1_s1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 80
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "conv3_1x1_s1_bn"
  type: "BatchNorm"
  bottom: "conv3_1x1_s1"
  top: "conv3_1x1_s1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv3_1x1_s1_scale"
  type: "Scale"
  bottom: "conv3_1x1_s1"
  top: "conv3_1x1_s1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_1x1_s1_relu"
  type: "ReLU"
  bottom: "conv3_1x1_s1"
  top: "conv3_1x1_s1"
}
layer {
  name: "conv4_3x3_s1"
  type: "Convolution"
  bottom: "conv3_1x1_s1"
  top: "conv4_3x3_s1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "conv4_3x3_s1_bn"
  type: "BatchNorm"
  bottom: "conv4_3x3_s1"
  top: "conv4_3x3_s1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv4_3x3_s1_scale"
  type: "Scale"
  bottom: "conv4_3x3_s1"
  top: "conv4_3x3_s1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_3x3_s1_relu"
  type: "ReLU"
  bottom: "conv4_3x3_s1"
  top: "conv4_3x3_s1"
}
layer {
  name: "pool_5a"
  type: "Pooling"
  bottom: "conv4_3x3_s1"
  top: "pool_5a"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "mixed_5b_branch_0_1x1"
  type: "Convolution"
  bottom: "pool_5a"
  top: "mixed_5b_branch_0_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 96
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "mixed_5b_branch_0_1x1_bn"
  type: "BatchNorm"
  bottom: "mixed_5b_branch_0_1x1"
  top: "mixed_5b_branch_0_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "mixed_5b_branch_0_1x1_scale"
  type: "Scale"
  bottom: "mixed_5b_branch_0_1x1"
  top: "mixed_5b_branch_0_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "mixed_5b_branch_0_1x1_relu"
  type: "ReLU"
  bottom: "mixed_5b_branch_0_1x1"
  top: "mixed_5b_branch_0_1x1"
}
layer {
  name: "mixed_5b_branch_1_1x1"
  type: "Convolution"
  bottom: "pool_5a"
  top: "mixed_5b_branch_1_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "mixed_5b_branch_1_1x1_bn"
  type: "BatchNorm"
  bottom: "mixed_5b_branch_1_1x1"
  top: "mixed_5b_branch_1_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "mixed_5b_branch_1_1x1_scale"
  type: "Scale"
  bottom: "mixed_5b_branch_1_1x1"
  top: "mixed_5b_branch_1_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "mixed_5b_branch_1_1x1_relu"
  type: "ReLU"
  bottom: "mixed_5b_branch_1_1x1"
  top: "mixed_5b_branch_1_1x1"
}
layer {
  name: "mixed_5b_branch_1_5x5"
  type: "Convolution"
  bottom: "mixed_5b_branch_1_1x1"
  top: "mixed_5b_branch_1_5x5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "mixed_5b_branch_1_5x5_bn"
  type: "BatchNorm"
  bottom: "mixed_5b_branch_1_5x5"
  top: "mixed_5b_branch_1_5x5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "mixed_5b_branch_1_5x5_scale"
  type: "Scale"
  bottom: "mixed_5b_branch_1_5x5"
  top: "mixed_5b_branch_1_5x5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "mixed_5b_branch_1_5x5_relu"
  type: "ReLU"
  bottom: "mixed_5b_branch_1_5x5"
  top: "mixed_5b_branch_1_5x5"
}
layer {
  name: "mixed_5b_branch_2_1x1"
  type: "Convolution"
  bottom: "pool_5a"
  top: "mixed_5b_branch_2_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "mixed_5b_branch_2_1x1_bn"
  type: "BatchNorm"
  bottom: "mixed_5b_branch_2_1x1"
  top: "mixed_5b_branch_2_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "mixed_5b_branch_2_1x1_scale"
  type: "Scale"
  bottom: "mixed_5b_branch_2_1x1"
  top: "mixed_5b_branch_2_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "mixed_5b_branch_2_1x1_relu"
  type: "ReLU"
  bottom: "mixed_5b_branch_2_1x1"
  top: "mixed_5b_branch_2_1x1"
}
layer {
  name: "mixed_5b_branch_2_3x3"
  type: "Convolution"
  bottom: "mixed_5b_branch_2_1x1"
  top: "mixed_5b_branch_2_3x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 96
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "mixed_5b_branch_2_3x3_bn"
  type: "BatchNorm"
  bottom: "mixed_5b_branch_2_3x3"
  top: "mixed_5b_branch_2_3x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "mixed_5b_branch_2_3x3_scale"
  type: "Scale"
  bottom: "mixed_5b_branch_2_3x3"
  top: "mixed_5b_branch_2_3x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "mixed_5b_branch_2_3x3_relu"
  type: "ReLU"
  bottom: "mixed_5b_branch_2_3x3"
  top: "mixed_5b_branch_2_3x3"
}
layer {
  name: "mixed_5b_branch_2_3x3_1"
  type: "Convolution"
  bottom: "mixed_5b_branch_2_3x3"
  top: "mixed_5b_branch_2_3x3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 96
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "mixed_5b_branch_2_3x3_1_bn"
  type: "BatchNorm"
  bottom: "mixed_5b_branch_2_3x3_1"
  top: "mixed_5b_branch_2_3x3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "mixed_5b_branch_2_3x3_1_scale"
  type: "Scale"
  bottom: "mixed_5b_branch_2_3x3_1"
  top: "mixed_5b_branch_2_3x3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "mixed_5b_branch_2_3x3_1_relu"
  type: "ReLU"
  bottom: "mixed_5b_branch_2_3x3_1"
  top: "mixed_5b_branch_2_3x3_1"
}
layer {
  name: "mixed_5b_branch_3_pool"
  type: "Pooling"
  bottom: "pool_5a"
  top: "mixed_5b_branch_3_pool"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "mixed_5b_branch_3_1x1"
  type: "Convolution"
  bottom: "mixed_5b_branch_3_pool"
  top: "mixed_5b_branch_3_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "mixed_5b_branch_3_1x1_bn"
  type: "BatchNorm"
  bottom: "mixed_5b_branch_3_1x1"
  top: "mixed_5b_branch_3_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "mixed_5b_branch_3_1x1_scale"
  type: "Scale"
  bottom: "mixed_5b_branch_3_1x1"
  top: "mixed_5b_branch_3_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "mixed_5b_branch_3_1x1_relu"
  type: "ReLU"
  bottom: "mixed_5b_branch_3_1x1"
  top: "mixed_5b_branch_3_1x1"
}
layer {
  name: "mixed_5b"
  type: "Concat"
  bottom: "mixed_5b_branch_0_1x1"
  bottom: "mixed_5b_branch_1_5x5"
  bottom: "mixed_5b_branch_2_3x3_1"
  bottom: "mixed_5b_branch_3_1x1"
  top: "mixed_5b"
}
layer {
  name: "a1_1x1"
  type: "Convolution"
  bottom: "mixed_5b"
  top: "a1_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a1_1x1_bn"
  type: "BatchNorm"
  bottom: "a1_1x1"
  top: "a1_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a1_1x1_scale"
  type: "Scale"
  bottom: "a1_1x1"
  top: "a1_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a1_1x1_relu"
  type: "ReLU"
  bottom: "a1_1x1"
  top: "a1_1x1"
}
layer {
  name: "a1_3x3_reduce"
  type: "Convolution"
  bottom: "mixed_5b"
  top: "a1_3x3_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a1_3x3_reduce_bn"
  type: "BatchNorm"
  bottom: "a1_3x3_reduce"
  top: "a1_3x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a1_3x3_reduce_scale"
  type: "Scale"
  bottom: "a1_3x3_reduce"
  top: "a1_3x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a1_3x3_reduce_relu"
  type: "ReLU"
  bottom: "a1_3x3_reduce"
  top: "a1_3x3_reduce"
}
layer {
  name: "a1_3x3"
  type: "Convolution"
  bottom: "a1_3x3_reduce"
  top: "a1_3x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a1_3x3_bn"
  type: "BatchNorm"
  bottom: "a1_3x3"
  top: "a1_3x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a1_3x3_scale"
  type: "Scale"
  bottom: "a1_3x3"
  top: "a1_3x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a1_3x3_relu"
  type: "ReLU"
  bottom: "a1_3x3"
  top: "a1_3x3"
}
layer {
  name: "a1_3x3_2_reduce"
  type: "Convolution"
  bottom: "mixed_5b"
  top: "a1_3x3_2_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a1_3x3_2_reduce_bn"
  type: "BatchNorm"
  bottom: "a1_3x3_2_reduce"
  top: "a1_3x3_2_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a1_3x3_2_reduce_scale"
  type: "Scale"
  bottom: "a1_3x3_2_reduce"
  top: "a1_3x3_2_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a1_3x3_2_reduce_relu"
  type: "ReLU"
  bottom: "a1_3x3_2_reduce"
  top: "a1_3x3_2_reduce"
}
layer {
  name: "a1_3x3_2"
  type: "Convolution"
  bottom: "a1_3x3_2_reduce"
  top: "a1_3x3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a1_3x3_2_bn"
  type: "BatchNorm"
  bottom: "a1_3x3_2"
  top: "a1_3x3_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a1_3x3_2_scale"
  type: "Scale"
  bottom: "a1_3x3_2"
  top: "a1_3x3_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a1_3x3_2_relu"
  type: "ReLU"
  bottom: "a1_3x3_2"
  top: "a1_3x3_2"
}
layer {
  name: "a1_3x3_3"
  type: "Convolution"
  bottom: "a1_3x3_2"
  top: "a1_3x3_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a1_3x3_3_bn"
  type: "BatchNorm"
  bottom: "a1_3x3_3"
  top: "a1_3x3_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a1_3x3_3_scale"
  type: "Scale"
  bottom: "a1_3x3_3"
  top: "a1_3x3_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a1_3x3_3_relu"
  type: "ReLU"
  bottom: "a1_3x3_3"
  top: "a1_3x3_3"
}
layer {
  name: "a1_concat"
  type: "Concat"
  bottom: "a1_1x1"
  bottom: "a1_3x3"
  bottom: "a1_3x3_3"
  top: "a1_concat"
}
layer {
  name: "a1_1x1_2"
  type: "Convolution"
  bottom: "a1_concat"
  top: "a1_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 320
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "a1_1x1_2_power"
  type: "Power"
  bottom: "a1_1x1_2"
  top: "a1_1x1_2_power"
  power_param {
    power: 1
    scale: 0.17
    shift: 0
  }
}
layer {
  name: "a1_residual_eltwise"
  type: "Eltwise"
  bottom: "mixed_5b"
  bottom: "a1_1x1_2_power"
  top: "a1_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "a1_residual_eltwise_relu"
  type: "ReLU"
  bottom: "a1_residual_eltwise"
  top: "a1_residual_eltwise"
}
layer {
  name: "a2_1x1"
  type: "Convolution"
  bottom: "a1_residual_eltwise"
  top: "a2_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a2_1x1_bn"
  type: "BatchNorm"
  bottom: "a2_1x1"
  top: "a2_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a2_1x1_scale"
  type: "Scale"
  bottom: "a2_1x1"
  top: "a2_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a2_1x1_relu"
  type: "ReLU"
  bottom: "a2_1x1"
  top: "a2_1x1"
}
layer {
  name: "a2_3x3_reduce"
  type: "Convolution"
  bottom: "a1_residual_eltwise"
  top: "a2_3x3_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a2_3x3_reduce_bn"
  type: "BatchNorm"
  bottom: "a2_3x3_reduce"
  top: "a2_3x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a2_3x3_reduce_scale"
  type: "Scale"
  bottom: "a2_3x3_reduce"
  top: "a2_3x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a2_3x3_reduce_relu"
  type: "ReLU"
  bottom: "a2_3x3_reduce"
  top: "a2_3x3_reduce"
}
layer {
  name: "a2_3x3"
  type: "Convolution"
  bottom: "a2_3x3_reduce"
  top: "a2_3x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a2_3x3_bn"
  type: "BatchNorm"
  bottom: "a2_3x3"
  top: "a2_3x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a2_3x3_scale"
  type: "Scale"
  bottom: "a2_3x3"
  top: "a2_3x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a2_3x3_relu"
  type: "ReLU"
  bottom: "a2_3x3"
  top: "a2_3x3"
}
layer {
  name: "a2_3x3_2_reduce"
  type: "Convolution"
  bottom: "a1_residual_eltwise"
  top: "a2_3x3_2_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a2_3x3_2_reduce_bn"
  type: "BatchNorm"
  bottom: "a2_3x3_2_reduce"
  top: "a2_3x3_2_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a2_3x3_2_reduce_scale"
  type: "Scale"
  bottom: "a2_3x3_2_reduce"
  top: "a2_3x3_2_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a2_3x3_2_reduce_relu"
  type: "ReLU"
  bottom: "a2_3x3_2_reduce"
  top: "a2_3x3_2_reduce"
}
layer {
  name: "a2_3x3_2"
  type: "Convolution"
  bottom: "a2_3x3_2_reduce"
  top: "a2_3x3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a2_3x3_2_bn"
  type: "BatchNorm"
  bottom: "a2_3x3_2"
  top: "a2_3x3_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a2_3x3_2_scale"
  type: "Scale"
  bottom: "a2_3x3_2"
  top: "a2_3x3_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a2_3x3_2_relu"
  type: "ReLU"
  bottom: "a2_3x3_2"
  top: "a2_3x3_2"
}
layer {
  name: "a2_3x3_3"
  type: "Convolution"
  bottom: "a2_3x3_2"
  top: "a2_3x3_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a2_3x3_3_bn"
  type: "BatchNorm"
  bottom: "a2_3x3_3"
  top: "a2_3x3_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a2_3x3_3_scale"
  type: "Scale"
  bottom: "a2_3x3_3"
  top: "a2_3x3_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a2_3x3_3_relu"
  type: "ReLU"
  bottom: "a2_3x3_3"
  top: "a2_3x3_3"
}
layer {
  name: "a2_concat"
  type: "Concat"
  bottom: "a2_1x1"
  bottom: "a2_3x3"
  bottom: "a2_3x3_3"
  top: "a2_concat"
}
layer {
  name: "a2_1x1_2"
  type: "Convolution"
  bottom: "a2_concat"
  top: "a2_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 320
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "a2_1x1_2_power"
  type: "Power"
  bottom: "a2_1x1_2"
  top: "a2_1x1_2_power"
  power_param {
    power: 1
    scale: 0.17
    shift: 0
  }
}
layer {
  name: "a2_residual_eltwise"
  type: "Eltwise"
  bottom: "a1_residual_eltwise"
  bottom: "a2_1x1_2_power"
  top: "a2_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "a2_residual_eltwise_relu"
  type: "ReLU"
  bottom: "a2_residual_eltwise"
  top: "a2_residual_eltwise"
}
layer {
  name: "a3_1x1"
  type: "Convolution"
  bottom: "a2_residual_eltwise"
  top: "a3_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a3_1x1_bn"
  type: "BatchNorm"
  bottom: "a3_1x1"
  top: "a3_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a3_1x1_scale"
  type: "Scale"
  bottom: "a3_1x1"
  top: "a3_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a3_1x1_relu"
  type: "ReLU"
  bottom: "a3_1x1"
  top: "a3_1x1"
}
layer {
  name: "a3_3x3_reduce"
  type: "Convolution"
  bottom: "a2_residual_eltwise"
  top: "a3_3x3_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a3_3x3_reduce_bn"
  type: "BatchNorm"
  bottom: "a3_3x3_reduce"
  top: "a3_3x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a3_3x3_reduce_scale"
  type: "Scale"
  bottom: "a3_3x3_reduce"
  top: "a3_3x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a3_3x3_reduce_relu"
  type: "ReLU"
  bottom: "a3_3x3_reduce"
  top: "a3_3x3_reduce"
}
layer {
  name: "a3_3x3"
  type: "Convolution"
  bottom: "a3_3x3_reduce"
  top: "a3_3x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a3_3x3_bn"
  type: "BatchNorm"
  bottom: "a3_3x3"
  top: "a3_3x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a3_3x3_scale"
  type: "Scale"
  bottom: "a3_3x3"
  top: "a3_3x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a3_3x3_relu"
  type: "ReLU"
  bottom: "a3_3x3"
  top: "a3_3x3"
}
layer {
  name: "a3_3x3_2_reduce"
  type: "Convolution"
  bottom: "a2_residual_eltwise"
  top: "a3_3x3_2_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a3_3x3_2_reduce_bn"
  type: "BatchNorm"
  bottom: "a3_3x3_2_reduce"
  top: "a3_3x3_2_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a3_3x3_2_reduce_scale"
  type: "Scale"
  bottom: "a3_3x3_2_reduce"
  top: "a3_3x3_2_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a3_3x3_2_reduce_relu"
  type: "ReLU"
  bottom: "a3_3x3_2_reduce"
  top: "a3_3x3_2_reduce"
}
layer {
  name: "a3_3x3_2"
  type: "Convolution"
  bottom: "a3_3x3_2_reduce"
  top: "a3_3x3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a3_3x3_2_bn"
  type: "BatchNorm"
  bottom: "a3_3x3_2"
  top: "a3_3x3_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a3_3x3_2_scale"
  type: "Scale"
  bottom: "a3_3x3_2"
  top: "a3_3x3_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a3_3x3_2_relu"
  type: "ReLU"
  bottom: "a3_3x3_2"
  top: "a3_3x3_2"
}
layer {
  name: "a3_3x3_3"
  type: "Convolution"
  bottom: "a3_3x3_2"
  top: "a3_3x3_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a3_3x3_3_bn"
  type: "BatchNorm"
  bottom: "a3_3x3_3"
  top: "a3_3x3_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a3_3x3_3_scale"
  type: "Scale"
  bottom: "a3_3x3_3"
  top: "a3_3x3_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a3_3x3_3_relu"
  type: "ReLU"
  bottom: "a3_3x3_3"
  top: "a3_3x3_3"
}
layer {
  name: "a3_concat"
  type: "Concat"
  bottom: "a3_1x1"
  bottom: "a3_3x3"
  bottom: "a3_3x3_3"
  top: "a3_concat"
}
layer {
  name: "a3_1x1_2"
  type: "Convolution"
  bottom: "a3_concat"
  top: "a3_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 320
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "a3_1x1_2_power"
  type: "Power"
  bottom: "a3_1x1_2"
  top: "a3_1x1_2_power"
  power_param {
    power: 1
    scale: 0.17
    shift: 0
  }
}
layer {
  name: "a3_residual_eltwise"
  type: "Eltwise"
  bottom: "a2_residual_eltwise"
  bottom: "a3_1x1_2_power"
  top: "a3_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "a3_residual_eltwise_relu"
  type: "ReLU"
  bottom: "a3_residual_eltwise"
  top: "a3_residual_eltwise"
}
layer {
  name: "a4_1x1"
  type: "Convolution"
  bottom: "a3_residual_eltwise"
  top: "a4_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a4_1x1_bn"
  type: "BatchNorm"
  bottom: "a4_1x1"
  top: "a4_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a4_1x1_scale"
  type: "Scale"
  bottom: "a4_1x1"
  top: "a4_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a4_1x1_relu"
  type: "ReLU"
  bottom: "a4_1x1"
  top: "a4_1x1"
}
layer {
  name: "a4_3x3_reduce"
  type: "Convolution"
  bottom: "a3_residual_eltwise"
  top: "a4_3x3_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a4_3x3_reduce_bn"
  type: "BatchNorm"
  bottom: "a4_3x3_reduce"
  top: "a4_3x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a4_3x3_reduce_scale"
  type: "Scale"
  bottom: "a4_3x3_reduce"
  top: "a4_3x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a4_3x3_reduce_relu"
  type: "ReLU"
  bottom: "a4_3x3_reduce"
  top: "a4_3x3_reduce"
}
layer {
  name: "a4_3x3"
  type: "Convolution"
  bottom: "a4_3x3_reduce"
  top: "a4_3x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a4_3x3_bn"
  type: "BatchNorm"
  bottom: "a4_3x3"
  top: "a4_3x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a4_3x3_scale"
  type: "Scale"
  bottom: "a4_3x3"
  top: "a4_3x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a4_3x3_relu"
  type: "ReLU"
  bottom: "a4_3x3"
  top: "a4_3x3"
}
layer {
  name: "a4_3x3_2_reduce"
  type: "Convolution"
  bottom: "a3_residual_eltwise"
  top: "a4_3x3_2_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a4_3x3_2_reduce_bn"
  type: "BatchNorm"
  bottom: "a4_3x3_2_reduce"
  top: "a4_3x3_2_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a4_3x3_2_reduce_scale"
  type: "Scale"
  bottom: "a4_3x3_2_reduce"
  top: "a4_3x3_2_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a4_3x3_2_reduce_relu"
  type: "ReLU"
  bottom: "a4_3x3_2_reduce"
  top: "a4_3x3_2_reduce"
}
layer {
  name: "a4_3x3_2"
  type: "Convolution"
  bottom: "a4_3x3_2_reduce"
  top: "a4_3x3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a4_3x3_2_bn"
  type: "BatchNorm"
  bottom: "a4_3x3_2"
  top: "a4_3x3_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a4_3x3_2_scale"
  type: "Scale"
  bottom: "a4_3x3_2"
  top: "a4_3x3_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a4_3x3_2_relu"
  type: "ReLU"
  bottom: "a4_3x3_2"
  top: "a4_3x3_2"
}
layer {
  name: "a4_3x3_3"
  type: "Convolution"
  bottom: "a4_3x3_2"
  top: "a4_3x3_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a4_3x3_3_bn"
  type: "BatchNorm"
  bottom: "a4_3x3_3"
  top: "a4_3x3_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a4_3x3_3_scale"
  type: "Scale"
  bottom: "a4_3x3_3"
  top: "a4_3x3_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a4_3x3_3_relu"
  type: "ReLU"
  bottom: "a4_3x3_3"
  top: "a4_3x3_3"
}
layer {
  name: "a4_concat"
  type: "Concat"
  bottom: "a4_1x1"
  bottom: "a4_3x3"
  bottom: "a4_3x3_3"
  top: "a4_concat"
}
layer {
  name: "a4_1x1_2"
  type: "Convolution"
  bottom: "a4_concat"
  top: "a4_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 320
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "a4_1x1_2_power"
  type: "Power"
  bottom: "a4_1x1_2"
  top: "a4_1x1_2_power"
  power_param {
    power: 1
    scale: 0.17
    shift: 0
  }
}
layer {
  name: "a4_residual_eltwise"
  type: "Eltwise"
  bottom: "a3_residual_eltwise"
  bottom: "a4_1x1_2_power"
  top: "a4_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "a4_residual_eltwise_relu"
  type: "ReLU"
  bottom: "a4_residual_eltwise"
  top: "a4_residual_eltwise"
}
layer {
  name: "a5_1x1"
  type: "Convolution"
  bottom: "a4_residual_eltwise"
  top: "a5_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a5_1x1_bn"
  type: "BatchNorm"
  bottom: "a5_1x1"
  top: "a5_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a5_1x1_scale"
  type: "Scale"
  bottom: "a5_1x1"
  top: "a5_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a5_1x1_relu"
  type: "ReLU"
  bottom: "a5_1x1"
  top: "a5_1x1"
}
layer {
  name: "a5_3x3_reduce"
  type: "Convolution"
  bottom: "a4_residual_eltwise"
  top: "a5_3x3_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a5_3x3_reduce_bn"
  type: "BatchNorm"
  bottom: "a5_3x3_reduce"
  top: "a5_3x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a5_3x3_reduce_scale"
  type: "Scale"
  bottom: "a5_3x3_reduce"
  top: "a5_3x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a5_3x3_reduce_relu"
  type: "ReLU"
  bottom: "a5_3x3_reduce"
  top: "a5_3x3_reduce"
}
layer {
  name: "a5_3x3"
  type: "Convolution"
  bottom: "a5_3x3_reduce"
  top: "a5_3x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a5_3x3_bn"
  type: "BatchNorm"
  bottom: "a5_3x3"
  top: "a5_3x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a5_3x3_scale"
  type: "Scale"
  bottom: "a5_3x3"
  top: "a5_3x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a5_3x3_relu"
  type: "ReLU"
  bottom: "a5_3x3"
  top: "a5_3x3"
}
layer {
  name: "a5_3x3_2_reduce"
  type: "Convolution"
  bottom: "a4_residual_eltwise"
  top: "a5_3x3_2_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a5_3x3_2_reduce_bn"
  type: "BatchNorm"
  bottom: "a5_3x3_2_reduce"
  top: "a5_3x3_2_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a5_3x3_2_reduce_scale"
  type: "Scale"
  bottom: "a5_3x3_2_reduce"
  top: "a5_3x3_2_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a5_3x3_2_reduce_relu"
  type: "ReLU"
  bottom: "a5_3x3_2_reduce"
  top: "a5_3x3_2_reduce"
}
layer {
  name: "a5_3x3_2"
  type: "Convolution"
  bottom: "a5_3x3_2_reduce"
  top: "a5_3x3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a5_3x3_2_bn"
  type: "BatchNorm"
  bottom: "a5_3x3_2"
  top: "a5_3x3_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a5_3x3_2_scale"
  type: "Scale"
  bottom: "a5_3x3_2"
  top: "a5_3x3_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a5_3x3_2_relu"
  type: "ReLU"
  bottom: "a5_3x3_2"
  top: "a5_3x3_2"
}
layer {
  name: "a5_3x3_3"
  type: "Convolution"
  bottom: "a5_3x3_2"
  top: "a5_3x3_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a5_3x3_3_bn"
  type: "BatchNorm"
  bottom: "a5_3x3_3"
  top: "a5_3x3_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a5_3x3_3_scale"
  type: "Scale"
  bottom: "a5_3x3_3"
  top: "a5_3x3_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a5_3x3_3_relu"
  type: "ReLU"
  bottom: "a5_3x3_3"
  top: "a5_3x3_3"
}
layer {
  name: "a5_concat"
  type: "Concat"
  bottom: "a5_1x1"
  bottom: "a5_3x3"
  bottom: "a5_3x3_3"
  top: "a5_concat"
}
layer {
  name: "a5_1x1_2"
  type: "Convolution"
  bottom: "a5_concat"
  top: "a5_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 320
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "a5_1x1_2_power"
  type: "Power"
  bottom: "a5_1x1_2"
  top: "a5_1x1_2_power"
  power_param {
    power: 1
    scale: 0.17
    shift: 0
  }
}
layer {
  name: "a5_residual_eltwise"
  type: "Eltwise"
  bottom: "a4_residual_eltwise"
  bottom: "a5_1x1_2_power"
  top: "a5_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "a5_residual_eltwise_relu"
  type: "ReLU"
  bottom: "a5_residual_eltwise"
  top: "a5_residual_eltwise"
}
layer {
  name: "a6_1x1"
  type: "Convolution"
  bottom: "a5_residual_eltwise"
  top: "a6_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a6_1x1_bn"
  type: "BatchNorm"
  bottom: "a6_1x1"
  top: "a6_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a6_1x1_scale"
  type: "Scale"
  bottom: "a6_1x1"
  top: "a6_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a6_1x1_relu"
  type: "ReLU"
  bottom: "a6_1x1"
  top: "a6_1x1"
}
layer {
  name: "a6_3x3_reduce"
  type: "Convolution"
  bottom: "a5_residual_eltwise"
  top: "a6_3x3_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a6_3x3_reduce_bn"
  type: "BatchNorm"
  bottom: "a6_3x3_reduce"
  top: "a6_3x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a6_3x3_reduce_scale"
  type: "Scale"
  bottom: "a6_3x3_reduce"
  top: "a6_3x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a6_3x3_reduce_relu"
  type: "ReLU"
  bottom: "a6_3x3_reduce"
  top: "a6_3x3_reduce"
}
layer {
  name: "a6_3x3"
  type: "Convolution"
  bottom: "a6_3x3_reduce"
  top: "a6_3x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a6_3x3_bn"
  type: "BatchNorm"
  bottom: "a6_3x3"
  top: "a6_3x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a6_3x3_scale"
  type: "Scale"
  bottom: "a6_3x3"
  top: "a6_3x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a6_3x3_relu"
  type: "ReLU"
  bottom: "a6_3x3"
  top: "a6_3x3"
}
layer {
  name: "a6_3x3_2_reduce"
  type: "Convolution"
  bottom: "a5_residual_eltwise"
  top: "a6_3x3_2_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a6_3x3_2_reduce_bn"
  type: "BatchNorm"
  bottom: "a6_3x3_2_reduce"
  top: "a6_3x3_2_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a6_3x3_2_reduce_scale"
  type: "Scale"
  bottom: "a6_3x3_2_reduce"
  top: "a6_3x3_2_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a6_3x3_2_reduce_relu"
  type: "ReLU"
  bottom: "a6_3x3_2_reduce"
  top: "a6_3x3_2_reduce"
}
layer {
  name: "a6_3x3_2"
  type: "Convolution"
  bottom: "a6_3x3_2_reduce"
  top: "a6_3x3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a6_3x3_2_bn"
  type: "BatchNorm"
  bottom: "a6_3x3_2"
  top: "a6_3x3_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a6_3x3_2_scale"
  type: "Scale"
  bottom: "a6_3x3_2"
  top: "a6_3x3_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a6_3x3_2_relu"
  type: "ReLU"
  bottom: "a6_3x3_2"
  top: "a6_3x3_2"
}
layer {
  name: "a6_3x3_3"
  type: "Convolution"
  bottom: "a6_3x3_2"
  top: "a6_3x3_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a6_3x3_3_bn"
  type: "BatchNorm"
  bottom: "a6_3x3_3"
  top: "a6_3x3_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a6_3x3_3_scale"
  type: "Scale"
  bottom: "a6_3x3_3"
  top: "a6_3x3_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a6_3x3_3_relu"
  type: "ReLU"
  bottom: "a6_3x3_3"
  top: "a6_3x3_3"
}
layer {
  name: "a6_concat"
  type: "Concat"
  bottom: "a6_1x1"
  bottom: "a6_3x3"
  bottom: "a6_3x3_3"
  top: "a6_concat"
}
layer {
  name: "a6_1x1_2"
  type: "Convolution"
  bottom: "a6_concat"
  top: "a6_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 320
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "a6_1x1_2_power"
  type: "Power"
  bottom: "a6_1x1_2"
  top: "a6_1x1_2_power"
  power_param {
    power: 1
    scale: 0.17
    shift: 0
  }
}
layer {
  name: "a6_residual_eltwise"
  type: "Eltwise"
  bottom: "a5_residual_eltwise"
  bottom: "a6_1x1_2_power"
  top: "a6_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "a6_residual_eltwise_relu"
  type: "ReLU"
  bottom: "a6_residual_eltwise"
  top: "a6_residual_eltwise"
}
layer {
  name: "a7_1x1"
  type: "Convolution"
  bottom: "a6_residual_eltwise"
  top: "a7_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a7_1x1_bn"
  type: "BatchNorm"
  bottom: "a7_1x1"
  top: "a7_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a7_1x1_scale"
  type: "Scale"
  bottom: "a7_1x1"
  top: "a7_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a7_1x1_relu"
  type: "ReLU"
  bottom: "a7_1x1"
  top: "a7_1x1"
}
layer {
  name: "a7_3x3_reduce"
  type: "Convolution"
  bottom: "a6_residual_eltwise"
  top: "a7_3x3_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a7_3x3_reduce_bn"
  type: "BatchNorm"
  bottom: "a7_3x3_reduce"
  top: "a7_3x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a7_3x3_reduce_scale"
  type: "Scale"
  bottom: "a7_3x3_reduce"
  top: "a7_3x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a7_3x3_reduce_relu"
  type: "ReLU"
  bottom: "a7_3x3_reduce"
  top: "a7_3x3_reduce"
}
layer {
  name: "a7_3x3"
  type: "Convolution"
  bottom: "a7_3x3_reduce"
  top: "a7_3x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a7_3x3_bn"
  type: "BatchNorm"
  bottom: "a7_3x3"
  top: "a7_3x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a7_3x3_scale"
  type: "Scale"
  bottom: "a7_3x3"
  top: "a7_3x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a7_3x3_relu"
  type: "ReLU"
  bottom: "a7_3x3"
  top: "a7_3x3"
}
layer {
  name: "a7_3x3_2_reduce"
  type: "Convolution"
  bottom: "a6_residual_eltwise"
  top: "a7_3x3_2_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a7_3x3_2_reduce_bn"
  type: "BatchNorm"
  bottom: "a7_3x3_2_reduce"
  top: "a7_3x3_2_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a7_3x3_2_reduce_scale"
  type: "Scale"
  bottom: "a7_3x3_2_reduce"
  top: "a7_3x3_2_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a7_3x3_2_reduce_relu"
  type: "ReLU"
  bottom: "a7_3x3_2_reduce"
  top: "a7_3x3_2_reduce"
}
layer {
  name: "a7_3x3_2"
  type: "Convolution"
  bottom: "a7_3x3_2_reduce"
  top: "a7_3x3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a7_3x3_2_bn"
  type: "BatchNorm"
  bottom: "a7_3x3_2"
  top: "a7_3x3_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a7_3x3_2_scale"
  type: "Scale"
  bottom: "a7_3x3_2"
  top: "a7_3x3_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a7_3x3_2_relu"
  type: "ReLU"
  bottom: "a7_3x3_2"
  top: "a7_3x3_2"
}
layer {
  name: "a7_3x3_3"
  type: "Convolution"
  bottom: "a7_3x3_2"
  top: "a7_3x3_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a7_3x3_3_bn"
  type: "BatchNorm"
  bottom: "a7_3x3_3"
  top: "a7_3x3_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a7_3x3_3_scale"
  type: "Scale"
  bottom: "a7_3x3_3"
  top: "a7_3x3_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a7_3x3_3_relu"
  type: "ReLU"
  bottom: "a7_3x3_3"
  top: "a7_3x3_3"
}
layer {
  name: "a7_concat"
  type: "Concat"
  bottom: "a7_1x1"
  bottom: "a7_3x3"
  bottom: "a7_3x3_3"
  top: "a7_concat"
}
layer {
  name: "a7_1x1_2"
  type: "Convolution"
  bottom: "a7_concat"
  top: "a7_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 320
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "a7_1x1_2_power"
  type: "Power"
  bottom: "a7_1x1_2"
  top: "a7_1x1_2_power"
  power_param {
    power: 1
    scale: 0.17
    shift: 0
  }
}
layer {
  name: "a7_residual_eltwise"
  type: "Eltwise"
  bottom: "a6_residual_eltwise"
  bottom: "a7_1x1_2_power"
  top: "a7_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "a7_residual_eltwise_relu"
  type: "ReLU"
  bottom: "a7_residual_eltwise"
  top: "a7_residual_eltwise"
}
layer {
  name: "a8_1x1"
  type: "Convolution"
  bottom: "a7_residual_eltwise"
  top: "a8_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a8_1x1_bn"
  type: "BatchNorm"
  bottom: "a8_1x1"
  top: "a8_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a8_1x1_scale"
  type: "Scale"
  bottom: "a8_1x1"
  top: "a8_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a8_1x1_relu"
  type: "ReLU"
  bottom: "a8_1x1"
  top: "a8_1x1"
}
layer {
  name: "a8_3x3_reduce"
  type: "Convolution"
  bottom: "a7_residual_eltwise"
  top: "a8_3x3_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a8_3x3_reduce_bn"
  type: "BatchNorm"
  bottom: "a8_3x3_reduce"
  top: "a8_3x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a8_3x3_reduce_scale"
  type: "Scale"
  bottom: "a8_3x3_reduce"
  top: "a8_3x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a8_3x3_reduce_relu"
  type: "ReLU"
  bottom: "a8_3x3_reduce"
  top: "a8_3x3_reduce"
}
layer {
  name: "a8_3x3"
  type: "Convolution"
  bottom: "a8_3x3_reduce"
  top: "a8_3x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a8_3x3_bn"
  type: "BatchNorm"
  bottom: "a8_3x3"
  top: "a8_3x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a8_3x3_scale"
  type: "Scale"
  bottom: "a8_3x3"
  top: "a8_3x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a8_3x3_relu"
  type: "ReLU"
  bottom: "a8_3x3"
  top: "a8_3x3"
}
layer {
  name: "a8_3x3_2_reduce"
  type: "Convolution"
  bottom: "a7_residual_eltwise"
  top: "a8_3x3_2_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a8_3x3_2_reduce_bn"
  type: "BatchNorm"
  bottom: "a8_3x3_2_reduce"
  top: "a8_3x3_2_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a8_3x3_2_reduce_scale"
  type: "Scale"
  bottom: "a8_3x3_2_reduce"
  top: "a8_3x3_2_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a8_3x3_2_reduce_relu"
  type: "ReLU"
  bottom: "a8_3x3_2_reduce"
  top: "a8_3x3_2_reduce"
}
layer {
  name: "a8_3x3_2"
  type: "Convolution"
  bottom: "a8_3x3_2_reduce"
  top: "a8_3x3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a8_3x3_2_bn"
  type: "BatchNorm"
  bottom: "a8_3x3_2"
  top: "a8_3x3_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a8_3x3_2_scale"
  type: "Scale"
  bottom: "a8_3x3_2"
  top: "a8_3x3_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a8_3x3_2_relu"
  type: "ReLU"
  bottom: "a8_3x3_2"
  top: "a8_3x3_2"
}
layer {
  name: "a8_3x3_3"
  type: "Convolution"
  bottom: "a8_3x3_2"
  top: "a8_3x3_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a8_3x3_3_bn"
  type: "BatchNorm"
  bottom: "a8_3x3_3"
  top: "a8_3x3_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a8_3x3_3_scale"
  type: "Scale"
  bottom: "a8_3x3_3"
  top: "a8_3x3_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a8_3x3_3_relu"
  type: "ReLU"
  bottom: "a8_3x3_3"
  top: "a8_3x3_3"
}
layer {
  name: "a8_concat"
  type: "Concat"
  bottom: "a8_1x1"
  bottom: "a8_3x3"
  bottom: "a8_3x3_3"
  top: "a8_concat"
}
layer {
  name: "a8_1x1_2"
  type: "Convolution"
  bottom: "a8_concat"
  top: "a8_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 320
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "a8_1x1_2_power"
  type: "Power"
  bottom: "a8_1x1_2"
  top: "a8_1x1_2_power"
  power_param {
    power: 1
    scale: 0.17
    shift: 0
  }
}
layer {
  name: "a8_residual_eltwise"
  type: "Eltwise"
  bottom: "a7_residual_eltwise"
  bottom: "a8_1x1_2_power"
  top: "a8_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "a8_residual_eltwise_relu"
  type: "ReLU"
  bottom: "a8_residual_eltwise"
  top: "a8_residual_eltwise"
}
layer {
  name: "a9_1x1"
  type: "Convolution"
  bottom: "a8_residual_eltwise"
  top: "a9_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a9_1x1_bn"
  type: "BatchNorm"
  bottom: "a9_1x1"
  top: "a9_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a9_1x1_scale"
  type: "Scale"
  bottom: "a9_1x1"
  top: "a9_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a9_1x1_relu"
  type: "ReLU"
  bottom: "a9_1x1"
  top: "a9_1x1"
}
layer {
  name: "a9_3x3_reduce"
  type: "Convolution"
  bottom: "a8_residual_eltwise"
  top: "a9_3x3_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a9_3x3_reduce_bn"
  type: "BatchNorm"
  bottom: "a9_3x3_reduce"
  top: "a9_3x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a9_3x3_reduce_scale"
  type: "Scale"
  bottom: "a9_3x3_reduce"
  top: "a9_3x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a9_3x3_reduce_relu"
  type: "ReLU"
  bottom: "a9_3x3_reduce"
  top: "a9_3x3_reduce"
}
layer {
  name: "a9_3x3"
  type: "Convolution"
  bottom: "a9_3x3_reduce"
  top: "a9_3x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a9_3x3_bn"
  type: "BatchNorm"
  bottom: "a9_3x3"
  top: "a9_3x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a9_3x3_scale"
  type: "Scale"
  bottom: "a9_3x3"
  top: "a9_3x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a9_3x3_relu"
  type: "ReLU"
  bottom: "a9_3x3"
  top: "a9_3x3"
}
layer {
  name: "a9_3x3_2_reduce"
  type: "Convolution"
  bottom: "a8_residual_eltwise"
  top: "a9_3x3_2_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a9_3x3_2_reduce_bn"
  type: "BatchNorm"
  bottom: "a9_3x3_2_reduce"
  top: "a9_3x3_2_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a9_3x3_2_reduce_scale"
  type: "Scale"
  bottom: "a9_3x3_2_reduce"
  top: "a9_3x3_2_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a9_3x3_2_reduce_relu"
  type: "ReLU"
  bottom: "a9_3x3_2_reduce"
  top: "a9_3x3_2_reduce"
}
layer {
  name: "a9_3x3_2"
  type: "Convolution"
  bottom: "a9_3x3_2_reduce"
  top: "a9_3x3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a9_3x3_2_bn"
  type: "BatchNorm"
  bottom: "a9_3x3_2"
  top: "a9_3x3_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a9_3x3_2_scale"
  type: "Scale"
  bottom: "a9_3x3_2"
  top: "a9_3x3_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a9_3x3_2_relu"
  type: "ReLU"
  bottom: "a9_3x3_2"
  top: "a9_3x3_2"
}
layer {
  name: "a9_3x3_3"
  type: "Convolution"
  bottom: "a9_3x3_2"
  top: "a9_3x3_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a9_3x3_3_bn"
  type: "BatchNorm"
  bottom: "a9_3x3_3"
  top: "a9_3x3_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a9_3x3_3_scale"
  type: "Scale"
  bottom: "a9_3x3_3"
  top: "a9_3x3_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a9_3x3_3_relu"
  type: "ReLU"
  bottom: "a9_3x3_3"
  top: "a9_3x3_3"
}
layer {
  name: "a9_concat"
  type: "Concat"
  bottom: "a9_1x1"
  bottom: "a9_3x3"
  bottom: "a9_3x3_3"
  top: "a9_concat"
}
layer {
  name: "a9_1x1_2"
  type: "Convolution"
  bottom: "a9_concat"
  top: "a9_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 320
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "a9_1x1_2_power"
  type: "Power"
  bottom: "a9_1x1_2"
  top: "a9_1x1_2_power"
  power_param {
    power: 1
    scale: 0.17
    shift: 0
  }
}
layer {
  name: "a9_residual_eltwise"
  type: "Eltwise"
  bottom: "a8_residual_eltwise"
  bottom: "a9_1x1_2_power"
  top: "a9_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "a9_residual_eltwise_relu"
  type: "ReLU"
  bottom: "a9_residual_eltwise"
  top: "a9_residual_eltwise"
}
layer {
  name: "a10_1x1"
  type: "Convolution"
  bottom: "a9_residual_eltwise"
  top: "a10_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a10_1x1_bn"
  type: "BatchNorm"
  bottom: "a10_1x1"
  top: "a10_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a10_1x1_scale"
  type: "Scale"
  bottom: "a10_1x1"
  top: "a10_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a10_1x1_relu"
  type: "ReLU"
  bottom: "a10_1x1"
  top: "a10_1x1"
}
layer {
  name: "a10_3x3_reduce"
  type: "Convolution"
  bottom: "a9_residual_eltwise"
  top: "a10_3x3_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a10_3x3_reduce_bn"
  type: "BatchNorm"
  bottom: "a10_3x3_reduce"
  top: "a10_3x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a10_3x3_reduce_scale"
  type: "Scale"
  bottom: "a10_3x3_reduce"
  top: "a10_3x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a10_3x3_reduce_relu"
  type: "ReLU"
  bottom: "a10_3x3_reduce"
  top: "a10_3x3_reduce"
}
layer {
  name: "a10_3x3"
  type: "Convolution"
  bottom: "a10_3x3_reduce"
  top: "a10_3x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a10_3x3_bn"
  type: "BatchNorm"
  bottom: "a10_3x3"
  top: "a10_3x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a10_3x3_scale"
  type: "Scale"
  bottom: "a10_3x3"
  top: "a10_3x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a10_3x3_relu"
  type: "ReLU"
  bottom: "a10_3x3"
  top: "a10_3x3"
}
layer {
  name: "a10_3x3_2_reduce"
  type: "Convolution"
  bottom: "a9_residual_eltwise"
  top: "a10_3x3_2_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a10_3x3_2_reduce_bn"
  type: "BatchNorm"
  bottom: "a10_3x3_2_reduce"
  top: "a10_3x3_2_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a10_3x3_2_reduce_scale"
  type: "Scale"
  bottom: "a10_3x3_2_reduce"
  top: "a10_3x3_2_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a10_3x3_2_reduce_relu"
  type: "ReLU"
  bottom: "a10_3x3_2_reduce"
  top: "a10_3x3_2_reduce"
}
layer {
  name: "a10_3x3_2"
  type: "Convolution"
  bottom: "a10_3x3_2_reduce"
  top: "a10_3x3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 48
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a10_3x3_2_bn"
  type: "BatchNorm"
  bottom: "a10_3x3_2"
  top: "a10_3x3_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a10_3x3_2_scale"
  type: "Scale"
  bottom: "a10_3x3_2"
  top: "a10_3x3_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a10_3x3_2_relu"
  type: "ReLU"
  bottom: "a10_3x3_2"
  top: "a10_3x3_2"
}
layer {
  name: "a10_3x3_3"
  type: "Convolution"
  bottom: "a10_3x3_2"
  top: "a10_3x3_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 64
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "a10_3x3_3_bn"
  type: "BatchNorm"
  bottom: "a10_3x3_3"
  top: "a10_3x3_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "a10_3x3_3_scale"
  type: "Scale"
  bottom: "a10_3x3_3"
  top: "a10_3x3_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "a10_3x3_3_relu"
  type: "ReLU"
  bottom: "a10_3x3_3"
  top: "a10_3x3_3"
}
layer {
  name: "a10_concat"
  type: "Concat"
  bottom: "a10_1x1"
  bottom: "a10_3x3"
  bottom: "a10_3x3_3"
  top: "a10_concat"
}
layer {
  name: "a10_1x1_2"
  type: "Convolution"
  bottom: "a10_concat"
  top: "a10_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 320
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "a10_1x1_2_power"
  type: "Power"
  bottom: "a10_1x1_2"
  top: "a10_1x1_2_power"
  power_param {
    power: 1
    scale: 0.17
    shift: 0
  }
}
layer {
  name: "a10_residual_eltwise"
  type: "Eltwise"
  bottom: "a9_residual_eltwise"
  bottom: "a10_1x1_2_power"
  top: "a10_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "a10_residual_eltwise_relu"
  type: "ReLU"
  bottom: "a10_residual_eltwise"
  top: "a10_residual_eltwise"
}
layer {
  name: "reduction_a_pool"
  type: "Pooling"
  bottom: "a10_residual_eltwise"
  top: "reduction_a_pool"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "reduction_a_3x3"
  type: "Convolution"
  bottom: "a10_residual_eltwise"
  top: "reduction_a_3x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 384
    bias_term: false
    pad: 0
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "reduction_a_3x3_bn"
  type: "BatchNorm"
  bottom: "reduction_a_3x3"
  top: "reduction_a_3x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "reduction_a_3x3_scale"
  type: "Scale"
  bottom: "reduction_a_3x3"
  top: "reduction_a_3x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "reduction_a_3x3_relu"
  type: "ReLU"
  bottom: "reduction_a_3x3"
  top: "reduction_a_3x3"
}
layer {
  name: "reduction_a_3x3_2_reduce"
  type: "Convolution"
  bottom: "a10_residual_eltwise"
  top: "reduction_a_3x3_2_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "reduction_a_3x3_2_reduce_bn"
  type: "BatchNorm"
  bottom: "reduction_a_3x3_2_reduce"
  top: "reduction_a_3x3_2_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "reduction_a_3x3_2_reduce_scale"
  type: "Scale"
  bottom: "reduction_a_3x3_2_reduce"
  top: "reduction_a_3x3_2_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "reduction_a_3x3_2_reduce_relu"
  type: "ReLU"
  bottom: "reduction_a_3x3_2_reduce"
  top: "reduction_a_3x3_2_reduce"
}
layer {
  name: "reduction_a_3x3_2"
  type: "Convolution"
  bottom: "reduction_a_3x3_2_reduce"
  top: "reduction_a_3x3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "reduction_a_3x3_2_bn"
  type: "BatchNorm"
  bottom: "reduction_a_3x3_2"
  top: "reduction_a_3x3_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "reduction_a_3x3_2_scale"
  type: "Scale"
  bottom: "reduction_a_3x3_2"
  top: "reduction_a_3x3_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "reduction_a_3x3_2_relu"
  type: "ReLU"
  bottom: "reduction_a_3x3_2"
  top: "reduction_a_3x3_2"
}
layer {
  name: "reduction_a_3x3_3"
  type: "Convolution"
  bottom: "reduction_a_3x3_2"
  top: "reduction_a_3x3_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 384
    bias_term: false
    pad: 0
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "reduction_a_3x3_3_bn"
  type: "BatchNorm"
  bottom: "reduction_a_3x3_3"
  top: "reduction_a_3x3_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "reduction_a_3x3_3_scale"
  type: "Scale"
  bottom: "reduction_a_3x3_3"
  top: "reduction_a_3x3_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "reduction_a_3x3_3_relu"
  type: "ReLU"
  bottom: "reduction_a_3x3_3"
  top: "reduction_a_3x3_3"
}
layer {
  name: "reduction_a_concat"
  type: "Concat"
  bottom: "reduction_a_3x3"
  bottom: "reduction_a_3x3_3"
  bottom: "reduction_a_pool"
  top: "reduction_a_concat"
}
layer {
  name: "b1_1x1"
  type: "Convolution"
  bottom: "reduction_a_concat"
  top: "b1_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b1_1x1_bn"
  type: "BatchNorm"
  bottom: "b1_1x1"
  top: "b1_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b1_1x1_scale"
  type: "Scale"
  bottom: "b1_1x1"
  top: "b1_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b1_1x1_relu"
  type: "ReLU"
  bottom: "b1_1x1"
  top: "b1_1x1"
}
layer {
  name: "b1_1x7_reduce"
  type: "Convolution"
  bottom: "reduction_a_concat"
  top: "b1_1x7_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b1_1x7_reduce_bn"
  type: "BatchNorm"
  bottom: "b1_1x7_reduce"
  top: "b1_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b1_1x7_reduce_scale"
  type: "Scale"
  bottom: "b1_1x7_reduce"
  top: "b1_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b1_1x7_reduce_relu"
  type: "ReLU"
  bottom: "b1_1x7_reduce"
  top: "b1_1x7_reduce"
}
layer {
  name: "b1_1x7"
  type: "Convolution"
  bottom: "b1_1x7_reduce"
  top: "b1_1x7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 160
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "b1_1x7_bn"
  type: "BatchNorm"
  bottom: "b1_1x7"
  top: "b1_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b1_1x7_scale"
  type: "Scale"
  bottom: "b1_1x7"
  top: "b1_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b1_1x7_relu"
  type: "ReLU"
  bottom: "b1_1x7"
  top: "b1_1x7"
}
layer {
  name: "b1_7x1"
  type: "Convolution"
  bottom: "b1_1x7"
  top: "b1_7x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "b1_7x1_bn"
  type: "BatchNorm"
  bottom: "b1_7x1"
  top: "b1_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b1_7x1_scale"
  type: "Scale"
  bottom: "b1_7x1"
  top: "b1_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b1_7x1_relu"
  type: "ReLU"
  bottom: "b1_7x1"
  top: "b1_7x1"
}
layer {
  name: "b1_concat"
  type: "Concat"
  bottom: "b1_1x1"
  bottom: "b1_7x1"
  top: "b1_concat"
}
layer {
  name: "b1_1x1_2"
  type: "Convolution"
  bottom: "b1_concat"
  top: "b1_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1088
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "b1_1x1_2_power"
  type: "Power"
  bottom: "b1_1x1_2"
  top: "b1_1x1_2_power"
  power_param {
    power: 1
    scale: 0.1
    shift: 0
  }
}
layer {
  name: "b1_residual_eltwise"
  type: "Eltwise"
  bottom: "reduction_a_concat"
  bottom: "b1_1x1_2_power"
  top: "b1_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "b1_residual_eltwise_relu"
  type: "ReLU"
  bottom: "b1_residual_eltwise"
  top: "b1_residual_eltwise"
}
layer {
  name: "b2_1x1"
  type: "Convolution"
  bottom: "b1_residual_eltwise"
  top: "b2_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b2_1x1_bn"
  type: "BatchNorm"
  bottom: "b2_1x1"
  top: "b2_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b2_1x1_scale"
  type: "Scale"
  bottom: "b2_1x1"
  top: "b2_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b2_1x1_relu"
  type: "ReLU"
  bottom: "b2_1x1"
  top: "b2_1x1"
}
layer {
  name: "b2_1x7_reduce"
  type: "Convolution"
  bottom: "b1_residual_eltwise"
  top: "b2_1x7_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b2_1x7_reduce_bn"
  type: "BatchNorm"
  bottom: "b2_1x7_reduce"
  top: "b2_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b2_1x7_reduce_scale"
  type: "Scale"
  bottom: "b2_1x7_reduce"
  top: "b2_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b2_1x7_reduce_relu"
  type: "ReLU"
  bottom: "b2_1x7_reduce"
  top: "b2_1x7_reduce"
}
layer {
  name: "b2_1x7"
  type: "Convolution"
  bottom: "b2_1x7_reduce"
  top: "b2_1x7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 160
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "b2_1x7_bn"
  type: "BatchNorm"
  bottom: "b2_1x7"
  top: "b2_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b2_1x7_scale"
  type: "Scale"
  bottom: "b2_1x7"
  top: "b2_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b2_1x7_relu"
  type: "ReLU"
  bottom: "b2_1x7"
  top: "b2_1x7"
}
layer {
  name: "b2_7x1"
  type: "Convolution"
  bottom: "b2_1x7"
  top: "b2_7x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "b2_7x1_bn"
  type: "BatchNorm"
  bottom: "b2_7x1"
  top: "b2_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b2_7x1_scale"
  type: "Scale"
  bottom: "b2_7x1"
  top: "b2_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b2_7x1_relu"
  type: "ReLU"
  bottom: "b2_7x1"
  top: "b2_7x1"
}
layer {
  name: "b2_concat"
  type: "Concat"
  bottom: "b2_1x1"
  bottom: "b2_7x1"
  top: "b2_concat"
}
layer {
  name: "b2_1x1_2"
  type: "Convolution"
  bottom: "b2_concat"
  top: "b2_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1088
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "b2_1x1_2_power"
  type: "Power"
  bottom: "b2_1x1_2"
  top: "b2_1x1_2_power"
  power_param {
    power: 1
    scale: 0.1
    shift: 0
  }
}
layer {
  name: "b2_residual_eltwise"
  type: "Eltwise"
  bottom: "b1_residual_eltwise"
  bottom: "b2_1x1_2_power"
  top: "b2_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "b2_residual_eltwise_relu"
  type: "ReLU"
  bottom: "b2_residual_eltwise"
  top: "b2_residual_eltwise"
}
layer {
  name: "b3_1x1"
  type: "Convolution"
  bottom: "b2_residual_eltwise"
  top: "b3_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b3_1x1_bn"
  type: "BatchNorm"
  bottom: "b3_1x1"
  top: "b3_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b3_1x1_scale"
  type: "Scale"
  bottom: "b3_1x1"
  top: "b3_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b3_1x1_relu"
  type: "ReLU"
  bottom: "b3_1x1"
  top: "b3_1x1"
}
layer {
  name: "b3_1x7_reduce"
  type: "Convolution"
  bottom: "b2_residual_eltwise"
  top: "b3_1x7_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b3_1x7_reduce_bn"
  type: "BatchNorm"
  bottom: "b3_1x7_reduce"
  top: "b3_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b3_1x7_reduce_scale"
  type: "Scale"
  bottom: "b3_1x7_reduce"
  top: "b3_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b3_1x7_reduce_relu"
  type: "ReLU"
  bottom: "b3_1x7_reduce"
  top: "b3_1x7_reduce"
}
layer {
  name: "b3_1x7"
  type: "Convolution"
  bottom: "b3_1x7_reduce"
  top: "b3_1x7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 160
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "b3_1x7_bn"
  type: "BatchNorm"
  bottom: "b3_1x7"
  top: "b3_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b3_1x7_scale"
  type: "Scale"
  bottom: "b3_1x7"
  top: "b3_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b3_1x7_relu"
  type: "ReLU"
  bottom: "b3_1x7"
  top: "b3_1x7"
}
layer {
  name: "b3_7x1"
  type: "Convolution"
  bottom: "b3_1x7"
  top: "b3_7x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "b3_7x1_bn"
  type: "BatchNorm"
  bottom: "b3_7x1"
  top: "b3_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b3_7x1_scale"
  type: "Scale"
  bottom: "b3_7x1"
  top: "b3_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b3_7x1_relu"
  type: "ReLU"
  bottom: "b3_7x1"
  top: "b3_7x1"
}
layer {
  name: "b3_concat"
  type: "Concat"
  bottom: "b3_1x1"
  bottom: "b3_7x1"
  top: "b3_concat"
}
layer {
  name: "b3_1x1_2"
  type: "Convolution"
  bottom: "b3_concat"
  top: "b3_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1088
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "b3_1x1_2_power"
  type: "Power"
  bottom: "b3_1x1_2"
  top: "b3_1x1_2_power"
  power_param {
    power: 1
    scale: 0.1
    shift: 0
  }
}
layer {
  name: "b3_residual_eltwise"
  type: "Eltwise"
  bottom: "b2_residual_eltwise"
  bottom: "b3_1x1_2_power"
  top: "b3_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "b3_residual_eltwise_relu"
  type: "ReLU"
  bottom: "b3_residual_eltwise"
  top: "b3_residual_eltwise"
}
layer {
  name: "b4_1x1"
  type: "Convolution"
  bottom: "b3_residual_eltwise"
  top: "b4_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b4_1x1_bn"
  type: "BatchNorm"
  bottom: "b4_1x1"
  top: "b4_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b4_1x1_scale"
  type: "Scale"
  bottom: "b4_1x1"
  top: "b4_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b4_1x1_relu"
  type: "ReLU"
  bottom: "b4_1x1"
  top: "b4_1x1"
}
layer {
  name: "b4_1x7_reduce"
  type: "Convolution"
  bottom: "b3_residual_eltwise"
  top: "b4_1x7_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b4_1x7_reduce_bn"
  type: "BatchNorm"
  bottom: "b4_1x7_reduce"
  top: "b4_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b4_1x7_reduce_scale"
  type: "Scale"
  bottom: "b4_1x7_reduce"
  top: "b4_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b4_1x7_reduce_relu"
  type: "ReLU"
  bottom: "b4_1x7_reduce"
  top: "b4_1x7_reduce"
}
layer {
  name: "b4_1x7"
  type: "Convolution"
  bottom: "b4_1x7_reduce"
  top: "b4_1x7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 160
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "b4_1x7_bn"
  type: "BatchNorm"
  bottom: "b4_1x7"
  top: "b4_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b4_1x7_scale"
  type: "Scale"
  bottom: "b4_1x7"
  top: "b4_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b4_1x7_relu"
  type: "ReLU"
  bottom: "b4_1x7"
  top: "b4_1x7"
}
layer {
  name: "b4_7x1"
  type: "Convolution"
  bottom: "b4_1x7"
  top: "b4_7x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "b4_7x1_bn"
  type: "BatchNorm"
  bottom: "b4_7x1"
  top: "b4_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b4_7x1_scale"
  type: "Scale"
  bottom: "b4_7x1"
  top: "b4_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b4_7x1_relu"
  type: "ReLU"
  bottom: "b4_7x1"
  top: "b4_7x1"
}
layer {
  name: "b4_concat"
  type: "Concat"
  bottom: "b4_1x1"
  bottom: "b4_7x1"
  top: "b4_concat"
}
layer {
  name: "b4_1x1_2"
  type: "Convolution"
  bottom: "b4_concat"
  top: "b4_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1088
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "b4_1x1_2_power"
  type: "Power"
  bottom: "b4_1x1_2"
  top: "b4_1x1_2_power"
  power_param {
    power: 1
    scale: 0.1
    shift: 0
  }
}
layer {
  name: "b4_residual_eltwise"
  type: "Eltwise"
  bottom: "b3_residual_eltwise"
  bottom: "b4_1x1_2_power"
  top: "b4_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "b4_residual_eltwise_relu"
  type: "ReLU"
  bottom: "b4_residual_eltwise"
  top: "b4_residual_eltwise"
}
layer {
  name: "b5_1x1"
  type: "Convolution"
  bottom: "b4_residual_eltwise"
  top: "b5_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b5_1x1_bn"
  type: "BatchNorm"
  bottom: "b5_1x1"
  top: "b5_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b5_1x1_scale"
  type: "Scale"
  bottom: "b5_1x1"
  top: "b5_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b5_1x1_relu"
  type: "ReLU"
  bottom: "b5_1x1"
  top: "b5_1x1"
}
layer {
  name: "b5_1x7_reduce"
  type: "Convolution"
  bottom: "b4_residual_eltwise"
  top: "b5_1x7_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b5_1x7_reduce_bn"
  type: "BatchNorm"
  bottom: "b5_1x7_reduce"
  top: "b5_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b5_1x7_reduce_scale"
  type: "Scale"
  bottom: "b5_1x7_reduce"
  top: "b5_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b5_1x7_reduce_relu"
  type: "ReLU"
  bottom: "b5_1x7_reduce"
  top: "b5_1x7_reduce"
}
layer {
  name: "b5_1x7"
  type: "Convolution"
  bottom: "b5_1x7_reduce"
  top: "b5_1x7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 160
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "b5_1x7_bn"
  type: "BatchNorm"
  bottom: "b5_1x7"
  top: "b5_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b5_1x7_scale"
  type: "Scale"
  bottom: "b5_1x7"
  top: "b5_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b5_1x7_relu"
  type: "ReLU"
  bottom: "b5_1x7"
  top: "b5_1x7"
}
layer {
  name: "b5_7x1"
  type: "Convolution"
  bottom: "b5_1x7"
  top: "b5_7x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "b5_7x1_bn"
  type: "BatchNorm"
  bottom: "b5_7x1"
  top: "b5_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b5_7x1_scale"
  type: "Scale"
  bottom: "b5_7x1"
  top: "b5_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b5_7x1_relu"
  type: "ReLU"
  bottom: "b5_7x1"
  top: "b5_7x1"
}
layer {
  name: "b5_concat"
  type: "Concat"
  bottom: "b5_1x1"
  bottom: "b5_7x1"
  top: "b5_concat"
}
layer {
  name: "b5_1x1_2"
  type: "Convolution"
  bottom: "b5_concat"
  top: "b5_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1088
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "b5_1x1_2_power"
  type: "Power"
  bottom: "b5_1x1_2"
  top: "b5_1x1_2_power"
  power_param {
    power: 1
    scale: 0.1
    shift: 0
  }
}
layer {
  name: "b5_residual_eltwise"
  type: "Eltwise"
  bottom: "b4_residual_eltwise"
  bottom: "b5_1x1_2_power"
  top: "b5_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "b5_residual_eltwise_relu"
  type: "ReLU"
  bottom: "b5_residual_eltwise"
  top: "b5_residual_eltwise"
}
layer {
  name: "b6_1x1"
  type: "Convolution"
  bottom: "b5_residual_eltwise"
  top: "b6_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b6_1x1_bn"
  type: "BatchNorm"
  bottom: "b6_1x1"
  top: "b6_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b6_1x1_scale"
  type: "Scale"
  bottom: "b6_1x1"
  top: "b6_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b6_1x1_relu"
  type: "ReLU"
  bottom: "b6_1x1"
  top: "b6_1x1"
}
layer {
  name: "b6_1x7_reduce"
  type: "Convolution"
  bottom: "b5_residual_eltwise"
  top: "b6_1x7_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b6_1x7_reduce_bn"
  type: "BatchNorm"
  bottom: "b6_1x7_reduce"
  top: "b6_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b6_1x7_reduce_scale"
  type: "Scale"
  bottom: "b6_1x7_reduce"
  top: "b6_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b6_1x7_reduce_relu"
  type: "ReLU"
  bottom: "b6_1x7_reduce"
  top: "b6_1x7_reduce"
}
layer {
  name: "b6_1x7"
  type: "Convolution"
  bottom: "b6_1x7_reduce"
  top: "b6_1x7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 160
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "b6_1x7_bn"
  type: "BatchNorm"
  bottom: "b6_1x7"
  top: "b6_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b6_1x7_scale"
  type: "Scale"
  bottom: "b6_1x7"
  top: "b6_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b6_1x7_relu"
  type: "ReLU"
  bottom: "b6_1x7"
  top: "b6_1x7"
}
layer {
  name: "b6_7x1"
  type: "Convolution"
  bottom: "b6_1x7"
  top: "b6_7x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "b6_7x1_bn"
  type: "BatchNorm"
  bottom: "b6_7x1"
  top: "b6_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b6_7x1_scale"
  type: "Scale"
  bottom: "b6_7x1"
  top: "b6_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b6_7x1_relu"
  type: "ReLU"
  bottom: "b6_7x1"
  top: "b6_7x1"
}
layer {
  name: "b6_concat"
  type: "Concat"
  bottom: "b6_1x1"
  bottom: "b6_7x1"
  top: "b6_concat"
}
layer {
  name: "b6_1x1_2"
  type: "Convolution"
  bottom: "b6_concat"
  top: "b6_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1088
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "b6_1x1_2_power"
  type: "Power"
  bottom: "b6_1x1_2"
  top: "b6_1x1_2_power"
  power_param {
    power: 1
    scale: 0.1
    shift: 0
  }
}
layer {
  name: "b6_residual_eltwise"
  type: "Eltwise"
  bottom: "b5_residual_eltwise"
  bottom: "b6_1x1_2_power"
  top: "b6_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "b6_residual_eltwise_relu"
  type: "ReLU"
  bottom: "b6_residual_eltwise"
  top: "b6_residual_eltwise"
}
layer {
  name: "b7_1x1"
  type: "Convolution"
  bottom: "b6_residual_eltwise"
  top: "b7_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b7_1x1_bn"
  type: "BatchNorm"
  bottom: "b7_1x1"
  top: "b7_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b7_1x1_scale"
  type: "Scale"
  bottom: "b7_1x1"
  top: "b7_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b7_1x1_relu"
  type: "ReLU"
  bottom: "b7_1x1"
  top: "b7_1x1"
}
layer {
  name: "b7_1x7_reduce"
  type: "Convolution"
  bottom: "b6_residual_eltwise"
  top: "b7_1x7_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b7_1x7_reduce_bn"
  type: "BatchNorm"
  bottom: "b7_1x7_reduce"
  top: "b7_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b7_1x7_reduce_scale"
  type: "Scale"
  bottom: "b7_1x7_reduce"
  top: "b7_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b7_1x7_reduce_relu"
  type: "ReLU"
  bottom: "b7_1x7_reduce"
  top: "b7_1x7_reduce"
}
layer {
  name: "b7_1x7"
  type: "Convolution"
  bottom: "b7_1x7_reduce"
  top: "b7_1x7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 160
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "b7_1x7_bn"
  type: "BatchNorm"
  bottom: "b7_1x7"
  top: "b7_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b7_1x7_scale"
  type: "Scale"
  bottom: "b7_1x7"
  top: "b7_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b7_1x7_relu"
  type: "ReLU"
  bottom: "b7_1x7"
  top: "b7_1x7"
}
layer {
  name: "b7_7x1"
  type: "Convolution"
  bottom: "b7_1x7"
  top: "b7_7x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "b7_7x1_bn"
  type: "BatchNorm"
  bottom: "b7_7x1"
  top: "b7_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b7_7x1_scale"
  type: "Scale"
  bottom: "b7_7x1"
  top: "b7_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b7_7x1_relu"
  type: "ReLU"
  bottom: "b7_7x1"
  top: "b7_7x1"
}
layer {
  name: "b7_concat"
  type: "Concat"
  bottom: "b7_1x1"
  bottom: "b7_7x1"
  top: "b7_concat"
}
layer {
  name: "b7_1x1_2"
  type: "Convolution"
  bottom: "b7_concat"
  top: "b7_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1088
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "b7_1x1_2_power"
  type: "Power"
  bottom: "b7_1x1_2"
  top: "b7_1x1_2_power"
  power_param {
    power: 1
    scale: 0.1
    shift: 0
  }
}
layer {
  name: "b7_residual_eltwise"
  type: "Eltwise"
  bottom: "b6_residual_eltwise"
  bottom: "b7_1x1_2_power"
  top: "b7_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "b7_residual_eltwise_relu"
  type: "ReLU"
  bottom: "b7_residual_eltwise"
  top: "b7_residual_eltwise"
}
layer {
  name: "b8_1x1"
  type: "Convolution"
  bottom: "b7_residual_eltwise"
  top: "b8_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b8_1x1_bn"
  type: "BatchNorm"
  bottom: "b8_1x1"
  top: "b8_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b8_1x1_scale"
  type: "Scale"
  bottom: "b8_1x1"
  top: "b8_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b8_1x1_relu"
  type: "ReLU"
  bottom: "b8_1x1"
  top: "b8_1x1"
}
layer {
  name: "b8_1x7_reduce"
  type: "Convolution"
  bottom: "b7_residual_eltwise"
  top: "b8_1x7_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b8_1x7_reduce_bn"
  type: "BatchNorm"
  bottom: "b8_1x7_reduce"
  top: "b8_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b8_1x7_reduce_scale"
  type: "Scale"
  bottom: "b8_1x7_reduce"
  top: "b8_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b8_1x7_reduce_relu"
  type: "ReLU"
  bottom: "b8_1x7_reduce"
  top: "b8_1x7_reduce"
}
layer {
  name: "b8_1x7"
  type: "Convolution"
  bottom: "b8_1x7_reduce"
  top: "b8_1x7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 160
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "b8_1x7_bn"
  type: "BatchNorm"
  bottom: "b8_1x7"
  top: "b8_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b8_1x7_scale"
  type: "Scale"
  bottom: "b8_1x7"
  top: "b8_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b8_1x7_relu"
  type: "ReLU"
  bottom: "b8_1x7"
  top: "b8_1x7"
}
layer {
  name: "b8_7x1"
  type: "Convolution"
  bottom: "b8_1x7"
  top: "b8_7x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "b8_7x1_bn"
  type: "BatchNorm"
  bottom: "b8_7x1"
  top: "b8_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b8_7x1_scale"
  type: "Scale"
  bottom: "b8_7x1"
  top: "b8_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b8_7x1_relu"
  type: "ReLU"
  bottom: "b8_7x1"
  top: "b8_7x1"
}
layer {
  name: "b8_concat"
  type: "Concat"
  bottom: "b8_1x1"
  bottom: "b8_7x1"
  top: "b8_concat"
}
layer {
  name: "b8_1x1_2"
  type: "Convolution"
  bottom: "b8_concat"
  top: "b8_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1088
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "b8_1x1_2_power"
  type: "Power"
  bottom: "b8_1x1_2"
  top: "b8_1x1_2_power"
  power_param {
    power: 1
    scale: 0.1
    shift: 0
  }
}
layer {
  name: "b8_residual_eltwise"
  type: "Eltwise"
  bottom: "b7_residual_eltwise"
  bottom: "b8_1x1_2_power"
  top: "b8_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "b8_residual_eltwise_relu"
  type: "ReLU"
  bottom: "b8_residual_eltwise"
  top: "b8_residual_eltwise"
}
layer {
  name: "b9_1x1"
  type: "Convolution"
  bottom: "b8_residual_eltwise"
  top: "b9_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b9_1x1_bn"
  type: "BatchNorm"
  bottom: "b9_1x1"
  top: "b9_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b9_1x1_scale"
  type: "Scale"
  bottom: "b9_1x1"
  top: "b9_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b9_1x1_relu"
  type: "ReLU"
  bottom: "b9_1x1"
  top: "b9_1x1"
}
layer {
  name: "b9_1x7_reduce"
  type: "Convolution"
  bottom: "b8_residual_eltwise"
  top: "b9_1x7_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b9_1x7_reduce_bn"
  type: "BatchNorm"
  bottom: "b9_1x7_reduce"
  top: "b9_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b9_1x7_reduce_scale"
  type: "Scale"
  bottom: "b9_1x7_reduce"
  top: "b9_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b9_1x7_reduce_relu"
  type: "ReLU"
  bottom: "b9_1x7_reduce"
  top: "b9_1x7_reduce"
}
layer {
  name: "b9_1x7"
  type: "Convolution"
  bottom: "b9_1x7_reduce"
  top: "b9_1x7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 160
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "b9_1x7_bn"
  type: "BatchNorm"
  bottom: "b9_1x7"
  top: "b9_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b9_1x7_scale"
  type: "Scale"
  bottom: "b9_1x7"
  top: "b9_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b9_1x7_relu"
  type: "ReLU"
  bottom: "b9_1x7"
  top: "b9_1x7"
}
layer {
  name: "b9_7x1"
  type: "Convolution"
  bottom: "b9_1x7"
  top: "b9_7x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "b9_7x1_bn"
  type: "BatchNorm"
  bottom: "b9_7x1"
  top: "b9_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b9_7x1_scale"
  type: "Scale"
  bottom: "b9_7x1"
  top: "b9_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b9_7x1_relu"
  type: "ReLU"
  bottom: "b9_7x1"
  top: "b9_7x1"
}
layer {
  name: "b9_concat"
  type: "Concat"
  bottom: "b9_1x1"
  bottom: "b9_7x1"
  top: "b9_concat"
}
layer {
  name: "b9_1x1_2"
  type: "Convolution"
  bottom: "b9_concat"
  top: "b9_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1088
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "b9_1x1_2_power"
  type: "Power"
  bottom: "b9_1x1_2"
  top: "b9_1x1_2_power"
  power_param {
    power: 1
    scale: 0.1
    shift: 0
  }
}
layer {
  name: "b9_residual_eltwise"
  type: "Eltwise"
  bottom: "b8_residual_eltwise"
  bottom: "b9_1x1_2_power"
  top: "b9_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "b9_residual_eltwise_relu"
  type: "ReLU"
  bottom: "b9_residual_eltwise"
  top: "b9_residual_eltwise"
}
layer {
  name: "b10_1x1"
  type: "Convolution"
  bottom: "b9_residual_eltwise"
  top: "b10_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b10_1x1_bn"
  type: "BatchNorm"
  bottom: "b10_1x1"
  top: "b10_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b10_1x1_scale"
  type: "Scale"
  bottom: "b10_1x1"
  top: "b10_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b10_1x1_relu"
  type: "ReLU"
  bottom: "b10_1x1"
  top: "b10_1x1"
}
layer {
  name: "b10_1x7_reduce"
  type: "Convolution"
  bottom: "b9_residual_eltwise"
  top: "b10_1x7_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b10_1x7_reduce_bn"
  type: "BatchNorm"
  bottom: "b10_1x7_reduce"
  top: "b10_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b10_1x7_reduce_scale"
  type: "Scale"
  bottom: "b10_1x7_reduce"
  top: "b10_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b10_1x7_reduce_relu"
  type: "ReLU"
  bottom: "b10_1x7_reduce"
  top: "b10_1x7_reduce"
}
layer {
  name: "b10_1x7"
  type: "Convolution"
  bottom: "b10_1x7_reduce"
  top: "b10_1x7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 160
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "b10_1x7_bn"
  type: "BatchNorm"
  bottom: "b10_1x7"
  top: "b10_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b10_1x7_scale"
  type: "Scale"
  bottom: "b10_1x7"
  top: "b10_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b10_1x7_relu"
  type: "ReLU"
  bottom: "b10_1x7"
  top: "b10_1x7"
}
layer {
  name: "b10_7x1"
  type: "Convolution"
  bottom: "b10_1x7"
  top: "b10_7x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "b10_7x1_bn"
  type: "BatchNorm"
  bottom: "b10_7x1"
  top: "b10_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b10_7x1_scale"
  type: "Scale"
  bottom: "b10_7x1"
  top: "b10_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b10_7x1_relu"
  type: "ReLU"
  bottom: "b10_7x1"
  top: "b10_7x1"
}
layer {
  name: "b10_concat"
  type: "Concat"
  bottom: "b10_1x1"
  bottom: "b10_7x1"
  top: "b10_concat"
}
layer {
  name: "b10_1x1_2"
  type: "Convolution"
  bottom: "b10_concat"
  top: "b10_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1088
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "b10_1x1_2_power"
  type: "Power"
  bottom: "b10_1x1_2"
  top: "b10_1x1_2_power"
  power_param {
    power: 1
    scale: 0.1
    shift: 0
  }
}
layer {
  name: "b10_residual_eltwise"
  type: "Eltwise"
  bottom: "b9_residual_eltwise"
  bottom: "b10_1x1_2_power"
  top: "b10_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "b10_residual_eltwise_relu"
  type: "ReLU"
  bottom: "b10_residual_eltwise"
  top: "b10_residual_eltwise"
}
layer {
  name: "b11_1x1"
  type: "Convolution"
  bottom: "b10_residual_eltwise"
  top: "b11_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b11_1x1_bn"
  type: "BatchNorm"
  bottom: "b11_1x1"
  top: "b11_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b11_1x1_scale"
  type: "Scale"
  bottom: "b11_1x1"
  top: "b11_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b11_1x1_relu"
  type: "ReLU"
  bottom: "b11_1x1"
  top: "b11_1x1"
}
layer {
  name: "b11_1x7_reduce"
  type: "Convolution"
  bottom: "b10_residual_eltwise"
  top: "b11_1x7_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b11_1x7_reduce_bn"
  type: "BatchNorm"
  bottom: "b11_1x7_reduce"
  top: "b11_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b11_1x7_reduce_scale"
  type: "Scale"
  bottom: "b11_1x7_reduce"
  top: "b11_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b11_1x7_reduce_relu"
  type: "ReLU"
  bottom: "b11_1x7_reduce"
  top: "b11_1x7_reduce"
}
layer {
  name: "b11_1x7"
  type: "Convolution"
  bottom: "b11_1x7_reduce"
  top: "b11_1x7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 160
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "b11_1x7_bn"
  type: "BatchNorm"
  bottom: "b11_1x7"
  top: "b11_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b11_1x7_scale"
  type: "Scale"
  bottom: "b11_1x7"
  top: "b11_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b11_1x7_relu"
  type: "ReLU"
  bottom: "b11_1x7"
  top: "b11_1x7"
}
layer {
  name: "b11_7x1"
  type: "Convolution"
  bottom: "b11_1x7"
  top: "b11_7x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "b11_7x1_bn"
  type: "BatchNorm"
  bottom: "b11_7x1"
  top: "b11_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b11_7x1_scale"
  type: "Scale"
  bottom: "b11_7x1"
  top: "b11_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b11_7x1_relu"
  type: "ReLU"
  bottom: "b11_7x1"
  top: "b11_7x1"
}
layer {
  name: "b11_concat"
  type: "Concat"
  bottom: "b11_1x1"
  bottom: "b11_7x1"
  top: "b11_concat"
}
layer {
  name: "b11_1x1_2"
  type: "Convolution"
  bottom: "b11_concat"
  top: "b11_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1088
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "b11_1x1_2_power"
  type: "Power"
  bottom: "b11_1x1_2"
  top: "b11_1x1_2_power"
  power_param {
    power: 1
    scale: 0.1
    shift: 0
  }
}
layer {
  name: "b11_residual_eltwise"
  type: "Eltwise"
  bottom: "b10_residual_eltwise"
  bottom: "b11_1x1_2_power"
  top: "b11_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "b11_residual_eltwise_relu"
  type: "ReLU"
  bottom: "b11_residual_eltwise"
  top: "b11_residual_eltwise"
}
layer {
  name: "b12_1x1"
  type: "Convolution"
  bottom: "b11_residual_eltwise"
  top: "b12_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b12_1x1_bn"
  type: "BatchNorm"
  bottom: "b12_1x1"
  top: "b12_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b12_1x1_scale"
  type: "Scale"
  bottom: "b12_1x1"
  top: "b12_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b12_1x1_relu"
  type: "ReLU"
  bottom: "b12_1x1"
  top: "b12_1x1"
}
layer {
  name: "b12_1x7_reduce"
  type: "Convolution"
  bottom: "b11_residual_eltwise"
  top: "b12_1x7_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b12_1x7_reduce_bn"
  type: "BatchNorm"
  bottom: "b12_1x7_reduce"
  top: "b12_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b12_1x7_reduce_scale"
  type: "Scale"
  bottom: "b12_1x7_reduce"
  top: "b12_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b12_1x7_reduce_relu"
  type: "ReLU"
  bottom: "b12_1x7_reduce"
  top: "b12_1x7_reduce"
}
layer {
  name: "b12_1x7"
  type: "Convolution"
  bottom: "b12_1x7_reduce"
  top: "b12_1x7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 160
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "b12_1x7_bn"
  type: "BatchNorm"
  bottom: "b12_1x7"
  top: "b12_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b12_1x7_scale"
  type: "Scale"
  bottom: "b12_1x7"
  top: "b12_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b12_1x7_relu"
  type: "ReLU"
  bottom: "b12_1x7"
  top: "b12_1x7"
}
layer {
  name: "b12_7x1"
  type: "Convolution"
  bottom: "b12_1x7"
  top: "b12_7x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "b12_7x1_bn"
  type: "BatchNorm"
  bottom: "b12_7x1"
  top: "b12_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b12_7x1_scale"
  type: "Scale"
  bottom: "b12_7x1"
  top: "b12_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b12_7x1_relu"
  type: "ReLU"
  bottom: "b12_7x1"
  top: "b12_7x1"
}
layer {
  name: "b12_concat"
  type: "Concat"
  bottom: "b12_1x1"
  bottom: "b12_7x1"
  top: "b12_concat"
}
layer {
  name: "b12_1x1_2"
  type: "Convolution"
  bottom: "b12_concat"
  top: "b12_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1088
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "b12_1x1_2_power"
  type: "Power"
  bottom: "b12_1x1_2"
  top: "b12_1x1_2_power"
  power_param {
    power: 1
    scale: 0.1
    shift: 0
  }
}
layer {
  name: "b12_residual_eltwise"
  type: "Eltwise"
  bottom: "b11_residual_eltwise"
  bottom: "b12_1x1_2_power"
  top: "b12_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "b12_residual_eltwise_relu"
  type: "ReLU"
  bottom: "b12_residual_eltwise"
  top: "b12_residual_eltwise"
}
layer {
  name: "b13_1x1"
  type: "Convolution"
  bottom: "b12_residual_eltwise"
  top: "b13_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b13_1x1_bn"
  type: "BatchNorm"
  bottom: "b13_1x1"
  top: "b13_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b13_1x1_scale"
  type: "Scale"
  bottom: "b13_1x1"
  top: "b13_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b13_1x1_relu"
  type: "ReLU"
  bottom: "b13_1x1"
  top: "b13_1x1"
}
layer {
  name: "b13_1x7_reduce"
  type: "Convolution"
  bottom: "b12_residual_eltwise"
  top: "b13_1x7_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b13_1x7_reduce_bn"
  type: "BatchNorm"
  bottom: "b13_1x7_reduce"
  top: "b13_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b13_1x7_reduce_scale"
  type: "Scale"
  bottom: "b13_1x7_reduce"
  top: "b13_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b13_1x7_reduce_relu"
  type: "ReLU"
  bottom: "b13_1x7_reduce"
  top: "b13_1x7_reduce"
}
layer {
  name: "b13_1x7"
  type: "Convolution"
  bottom: "b13_1x7_reduce"
  top: "b13_1x7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 160
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "b13_1x7_bn"
  type: "BatchNorm"
  bottom: "b13_1x7"
  top: "b13_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b13_1x7_scale"
  type: "Scale"
  bottom: "b13_1x7"
  top: "b13_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b13_1x7_relu"
  type: "ReLU"
  bottom: "b13_1x7"
  top: "b13_1x7"
}
layer {
  name: "b13_7x1"
  type: "Convolution"
  bottom: "b13_1x7"
  top: "b13_7x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "b13_7x1_bn"
  type: "BatchNorm"
  bottom: "b13_7x1"
  top: "b13_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b13_7x1_scale"
  type: "Scale"
  bottom: "b13_7x1"
  top: "b13_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b13_7x1_relu"
  type: "ReLU"
  bottom: "b13_7x1"
  top: "b13_7x1"
}
layer {
  name: "b13_concat"
  type: "Concat"
  bottom: "b13_1x1"
  bottom: "b13_7x1"
  top: "b13_concat"
}
layer {
  name: "b13_1x1_2"
  type: "Convolution"
  bottom: "b13_concat"
  top: "b13_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1088
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "b13_1x1_2_power"
  type: "Power"
  bottom: "b13_1x1_2"
  top: "b13_1x1_2_power"
  power_param {
    power: 1
    scale: 0.1
    shift: 0
  }
}
layer {
  name: "b13_residual_eltwise"
  type: "Eltwise"
  bottom: "b12_residual_eltwise"
  bottom: "b13_1x1_2_power"
  top: "b13_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "b13_residual_eltwise_relu"
  type: "ReLU"
  bottom: "b13_residual_eltwise"
  top: "b13_residual_eltwise"
}
layer {
  name: "b14_1x1"
  type: "Convolution"
  bottom: "b13_residual_eltwise"
  top: "b14_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b14_1x1_bn"
  type: "BatchNorm"
  bottom: "b14_1x1"
  top: "b14_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b14_1x1_scale"
  type: "Scale"
  bottom: "b14_1x1"
  top: "b14_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b14_1x1_relu"
  type: "ReLU"
  bottom: "b14_1x1"
  top: "b14_1x1"
}
layer {
  name: "b14_1x7_reduce"
  type: "Convolution"
  bottom: "b13_residual_eltwise"
  top: "b14_1x7_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b14_1x7_reduce_bn"
  type: "BatchNorm"
  bottom: "b14_1x7_reduce"
  top: "b14_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b14_1x7_reduce_scale"
  type: "Scale"
  bottom: "b14_1x7_reduce"
  top: "b14_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b14_1x7_reduce_relu"
  type: "ReLU"
  bottom: "b14_1x7_reduce"
  top: "b14_1x7_reduce"
}
layer {
  name: "b14_1x7"
  type: "Convolution"
  bottom: "b14_1x7_reduce"
  top: "b14_1x7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 160
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "b14_1x7_bn"
  type: "BatchNorm"
  bottom: "b14_1x7"
  top: "b14_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b14_1x7_scale"
  type: "Scale"
  bottom: "b14_1x7"
  top: "b14_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b14_1x7_relu"
  type: "ReLU"
  bottom: "b14_1x7"
  top: "b14_1x7"
}
layer {
  name: "b14_7x1"
  type: "Convolution"
  bottom: "b14_1x7"
  top: "b14_7x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "b14_7x1_bn"
  type: "BatchNorm"
  bottom: "b14_7x1"
  top: "b14_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b14_7x1_scale"
  type: "Scale"
  bottom: "b14_7x1"
  top: "b14_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b14_7x1_relu"
  type: "ReLU"
  bottom: "b14_7x1"
  top: "b14_7x1"
}
layer {
  name: "b14_concat"
  type: "Concat"
  bottom: "b14_1x1"
  bottom: "b14_7x1"
  top: "b14_concat"
}
layer {
  name: "b14_1x1_2"
  type: "Convolution"
  bottom: "b14_concat"
  top: "b14_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1088
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "b14_1x1_2_power"
  type: "Power"
  bottom: "b14_1x1_2"
  top: "b14_1x1_2_power"
  power_param {
    power: 1
    scale: 0.1
    shift: 0
  }
}
layer {
  name: "b14_residual_eltwise"
  type: "Eltwise"
  bottom: "b13_residual_eltwise"
  bottom: "b14_1x1_2_power"
  top: "b14_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "b14_residual_eltwise_relu"
  type: "ReLU"
  bottom: "b14_residual_eltwise"
  top: "b14_residual_eltwise"
}
layer {
  name: "b15_1x1"
  type: "Convolution"
  bottom: "b14_residual_eltwise"
  top: "b15_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b15_1x1_bn"
  type: "BatchNorm"
  bottom: "b15_1x1"
  top: "b15_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b15_1x1_scale"
  type: "Scale"
  bottom: "b15_1x1"
  top: "b15_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b15_1x1_relu"
  type: "ReLU"
  bottom: "b15_1x1"
  top: "b15_1x1"
}
layer {
  name: "b15_1x7_reduce"
  type: "Convolution"
  bottom: "b14_residual_eltwise"
  top: "b15_1x7_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b15_1x7_reduce_bn"
  type: "BatchNorm"
  bottom: "b15_1x7_reduce"
  top: "b15_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b15_1x7_reduce_scale"
  type: "Scale"
  bottom: "b15_1x7_reduce"
  top: "b15_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b15_1x7_reduce_relu"
  type: "ReLU"
  bottom: "b15_1x7_reduce"
  top: "b15_1x7_reduce"
}
layer {
  name: "b15_1x7"
  type: "Convolution"
  bottom: "b15_1x7_reduce"
  top: "b15_1x7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 160
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "b15_1x7_bn"
  type: "BatchNorm"
  bottom: "b15_1x7"
  top: "b15_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b15_1x7_scale"
  type: "Scale"
  bottom: "b15_1x7"
  top: "b15_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b15_1x7_relu"
  type: "ReLU"
  bottom: "b15_1x7"
  top: "b15_1x7"
}
layer {
  name: "b15_7x1"
  type: "Convolution"
  bottom: "b15_1x7"
  top: "b15_7x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "b15_7x1_bn"
  type: "BatchNorm"
  bottom: "b15_7x1"
  top: "b15_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b15_7x1_scale"
  type: "Scale"
  bottom: "b15_7x1"
  top: "b15_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b15_7x1_relu"
  type: "ReLU"
  bottom: "b15_7x1"
  top: "b15_7x1"
}
layer {
  name: "b15_concat"
  type: "Concat"
  bottom: "b15_1x1"
  bottom: "b15_7x1"
  top: "b15_concat"
}
layer {
  name: "b15_1x1_2"
  type: "Convolution"
  bottom: "b15_concat"
  top: "b15_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1088
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "b15_1x1_2_power"
  type: "Power"
  bottom: "b15_1x1_2"
  top: "b15_1x1_2_power"
  power_param {
    power: 1
    scale: 0.1
    shift: 0
  }
}
layer {
  name: "b15_residual_eltwise"
  type: "Eltwise"
  bottom: "b14_residual_eltwise"
  bottom: "b15_1x1_2_power"
  top: "b15_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "b15_residual_eltwise_relu"
  type: "ReLU"
  bottom: "b15_residual_eltwise"
  top: "b15_residual_eltwise"
}
layer {
  name: "b16_1x1"
  type: "Convolution"
  bottom: "b15_residual_eltwise"
  top: "b16_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b16_1x1_bn"
  type: "BatchNorm"
  bottom: "b16_1x1"
  top: "b16_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b16_1x1_scale"
  type: "Scale"
  bottom: "b16_1x1"
  top: "b16_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b16_1x1_relu"
  type: "ReLU"
  bottom: "b16_1x1"
  top: "b16_1x1"
}
layer {
  name: "b16_1x7_reduce"
  type: "Convolution"
  bottom: "b15_residual_eltwise"
  top: "b16_1x7_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b16_1x7_reduce_bn"
  type: "BatchNorm"
  bottom: "b16_1x7_reduce"
  top: "b16_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b16_1x7_reduce_scale"
  type: "Scale"
  bottom: "b16_1x7_reduce"
  top: "b16_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b16_1x7_reduce_relu"
  type: "ReLU"
  bottom: "b16_1x7_reduce"
  top: "b16_1x7_reduce"
}
layer {
  name: "b16_1x7"
  type: "Convolution"
  bottom: "b16_1x7_reduce"
  top: "b16_1x7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 160
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "b16_1x7_bn"
  type: "BatchNorm"
  bottom: "b16_1x7"
  top: "b16_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b16_1x7_scale"
  type: "Scale"
  bottom: "b16_1x7"
  top: "b16_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b16_1x7_relu"
  type: "ReLU"
  bottom: "b16_1x7"
  top: "b16_1x7"
}
layer {
  name: "b16_7x1"
  type: "Convolution"
  bottom: "b16_1x7"
  top: "b16_7x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "b16_7x1_bn"
  type: "BatchNorm"
  bottom: "b16_7x1"
  top: "b16_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b16_7x1_scale"
  type: "Scale"
  bottom: "b16_7x1"
  top: "b16_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b16_7x1_relu"
  type: "ReLU"
  bottom: "b16_7x1"
  top: "b16_7x1"
}
layer {
  name: "b16_concat"
  type: "Concat"
  bottom: "b16_1x1"
  bottom: "b16_7x1"
  top: "b16_concat"
}
layer {
  name: "b16_1x1_2"
  type: "Convolution"
  bottom: "b16_concat"
  top: "b16_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1088
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "b16_1x1_2_power"
  type: "Power"
  bottom: "b16_1x1_2"
  top: "b16_1x1_2_power"
  power_param {
    power: 1
    scale: 0.1
    shift: 0
  }
}
layer {
  name: "b16_residual_eltwise"
  type: "Eltwise"
  bottom: "b15_residual_eltwise"
  bottom: "b16_1x1_2_power"
  top: "b16_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "b16_residual_eltwise_relu"
  type: "ReLU"
  bottom: "b16_residual_eltwise"
  top: "b16_residual_eltwise"
}
layer {
  name: "b17_1x1"
  type: "Convolution"
  bottom: "b16_residual_eltwise"
  top: "b17_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b17_1x1_bn"
  type: "BatchNorm"
  bottom: "b17_1x1"
  top: "b17_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b17_1x1_scale"
  type: "Scale"
  bottom: "b17_1x1"
  top: "b17_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b17_1x1_relu"
  type: "ReLU"
  bottom: "b17_1x1"
  top: "b17_1x1"
}
layer {
  name: "b17_1x7_reduce"
  type: "Convolution"
  bottom: "b16_residual_eltwise"
  top: "b17_1x7_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b17_1x7_reduce_bn"
  type: "BatchNorm"
  bottom: "b17_1x7_reduce"
  top: "b17_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b17_1x7_reduce_scale"
  type: "Scale"
  bottom: "b17_1x7_reduce"
  top: "b17_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b17_1x7_reduce_relu"
  type: "ReLU"
  bottom: "b17_1x7_reduce"
  top: "b17_1x7_reduce"
}
layer {
  name: "b17_1x7"
  type: "Convolution"
  bottom: "b17_1x7_reduce"
  top: "b17_1x7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 160
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "b17_1x7_bn"
  type: "BatchNorm"
  bottom: "b17_1x7"
  top: "b17_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b17_1x7_scale"
  type: "Scale"
  bottom: "b17_1x7"
  top: "b17_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b17_1x7_relu"
  type: "ReLU"
  bottom: "b17_1x7"
  top: "b17_1x7"
}
layer {
  name: "b17_7x1"
  type: "Convolution"
  bottom: "b17_1x7"
  top: "b17_7x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "b17_7x1_bn"
  type: "BatchNorm"
  bottom: "b17_7x1"
  top: "b17_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b17_7x1_scale"
  type: "Scale"
  bottom: "b17_7x1"
  top: "b17_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b17_7x1_relu"
  type: "ReLU"
  bottom: "b17_7x1"
  top: "b17_7x1"
}
layer {
  name: "b17_concat"
  type: "Concat"
  bottom: "b17_1x1"
  bottom: "b17_7x1"
  top: "b17_concat"
}
layer {
  name: "b17_1x1_2"
  type: "Convolution"
  bottom: "b17_concat"
  top: "b17_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1088
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "b17_1x1_2_power"
  type: "Power"
  bottom: "b17_1x1_2"
  top: "b17_1x1_2_power"
  power_param {
    power: 1
    scale: 0.1
    shift: 0
  }
}
layer {
  name: "b17_residual_eltwise"
  type: "Eltwise"
  bottom: "b16_residual_eltwise"
  bottom: "b17_1x1_2_power"
  top: "b17_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "b17_residual_eltwise_relu"
  type: "ReLU"
  bottom: "b17_residual_eltwise"
  top: "b17_residual_eltwise"
}
layer {
  name: "b18_1x1"
  type: "Convolution"
  bottom: "b17_residual_eltwise"
  top: "b18_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b18_1x1_bn"
  type: "BatchNorm"
  bottom: "b18_1x1"
  top: "b18_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b18_1x1_scale"
  type: "Scale"
  bottom: "b18_1x1"
  top: "b18_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b18_1x1_relu"
  type: "ReLU"
  bottom: "b18_1x1"
  top: "b18_1x1"
}
layer {
  name: "b18_1x7_reduce"
  type: "Convolution"
  bottom: "b17_residual_eltwise"
  top: "b18_1x7_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b18_1x7_reduce_bn"
  type: "BatchNorm"
  bottom: "b18_1x7_reduce"
  top: "b18_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b18_1x7_reduce_scale"
  type: "Scale"
  bottom: "b18_1x7_reduce"
  top: "b18_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b18_1x7_reduce_relu"
  type: "ReLU"
  bottom: "b18_1x7_reduce"
  top: "b18_1x7_reduce"
}
layer {
  name: "b18_1x7"
  type: "Convolution"
  bottom: "b18_1x7_reduce"
  top: "b18_1x7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 160
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "b18_1x7_bn"
  type: "BatchNorm"
  bottom: "b18_1x7"
  top: "b18_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b18_1x7_scale"
  type: "Scale"
  bottom: "b18_1x7"
  top: "b18_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b18_1x7_relu"
  type: "ReLU"
  bottom: "b18_1x7"
  top: "b18_1x7"
}
layer {
  name: "b18_7x1"
  type: "Convolution"
  bottom: "b18_1x7"
  top: "b18_7x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "b18_7x1_bn"
  type: "BatchNorm"
  bottom: "b18_7x1"
  top: "b18_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b18_7x1_scale"
  type: "Scale"
  bottom: "b18_7x1"
  top: "b18_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b18_7x1_relu"
  type: "ReLU"
  bottom: "b18_7x1"
  top: "b18_7x1"
}
layer {
  name: "b18_concat"
  type: "Concat"
  bottom: "b18_1x1"
  bottom: "b18_7x1"
  top: "b18_concat"
}
layer {
  name: "b18_1x1_2"
  type: "Convolution"
  bottom: "b18_concat"
  top: "b18_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1088
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "b18_1x1_2_power"
  type: "Power"
  bottom: "b18_1x1_2"
  top: "b18_1x1_2_power"
  power_param {
    power: 1
    scale: 0.1
    shift: 0
  }
}
layer {
  name: "b18_residual_eltwise"
  type: "Eltwise"
  bottom: "b17_residual_eltwise"
  bottom: "b18_1x1_2_power"
  top: "b18_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "b18_residual_eltwise_relu"
  type: "ReLU"
  bottom: "b18_residual_eltwise"
  top: "b18_residual_eltwise"
}
layer {
  name: "b19_1x1"
  type: "Convolution"
  bottom: "b18_residual_eltwise"
  top: "b19_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b19_1x1_bn"
  type: "BatchNorm"
  bottom: "b19_1x1"
  top: "b19_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b19_1x1_scale"
  type: "Scale"
  bottom: "b19_1x1"
  top: "b19_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b19_1x1_relu"
  type: "ReLU"
  bottom: "b19_1x1"
  top: "b19_1x1"
}
layer {
  name: "b19_1x7_reduce"
  type: "Convolution"
  bottom: "b18_residual_eltwise"
  top: "b19_1x7_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b19_1x7_reduce_bn"
  type: "BatchNorm"
  bottom: "b19_1x7_reduce"
  top: "b19_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b19_1x7_reduce_scale"
  type: "Scale"
  bottom: "b19_1x7_reduce"
  top: "b19_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b19_1x7_reduce_relu"
  type: "ReLU"
  bottom: "b19_1x7_reduce"
  top: "b19_1x7_reduce"
}
layer {
  name: "b19_1x7"
  type: "Convolution"
  bottom: "b19_1x7_reduce"
  top: "b19_1x7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 160
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "b19_1x7_bn"
  type: "BatchNorm"
  bottom: "b19_1x7"
  top: "b19_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b19_1x7_scale"
  type: "Scale"
  bottom: "b19_1x7"
  top: "b19_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b19_1x7_relu"
  type: "ReLU"
  bottom: "b19_1x7"
  top: "b19_1x7"
}
layer {
  name: "b19_7x1"
  type: "Convolution"
  bottom: "b19_1x7"
  top: "b19_7x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "b19_7x1_bn"
  type: "BatchNorm"
  bottom: "b19_7x1"
  top: "b19_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b19_7x1_scale"
  type: "Scale"
  bottom: "b19_7x1"
  top: "b19_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b19_7x1_relu"
  type: "ReLU"
  bottom: "b19_7x1"
  top: "b19_7x1"
}
layer {
  name: "b19_concat"
  type: "Concat"
  bottom: "b19_1x1"
  bottom: "b19_7x1"
  top: "b19_concat"
}
layer {
  name: "b19_1x1_2"
  type: "Convolution"
  bottom: "b19_concat"
  top: "b19_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1088
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "b19_1x1_2_power"
  type: "Power"
  bottom: "b19_1x1_2"
  top: "b19_1x1_2_power"
  power_param {
    power: 1
    scale: 0.1
    shift: 0
  }
}
layer {
  name: "b19_residual_eltwise"
  type: "Eltwise"
  bottom: "b18_residual_eltwise"
  bottom: "b19_1x1_2_power"
  top: "b19_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "b19_residual_eltwise_relu"
  type: "ReLU"
  bottom: "b19_residual_eltwise"
  top: "b19_residual_eltwise"
}
layer {
  name: "b20_1x1"
  type: "Convolution"
  bottom: "b19_residual_eltwise"
  top: "b20_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b20_1x1_bn"
  type: "BatchNorm"
  bottom: "b20_1x1"
  top: "b20_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b20_1x1_scale"
  type: "Scale"
  bottom: "b20_1x1"
  top: "b20_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b20_1x1_relu"
  type: "ReLU"
  bottom: "b20_1x1"
  top: "b20_1x1"
}
layer {
  name: "b20_1x7_reduce"
  type: "Convolution"
  bottom: "b19_residual_eltwise"
  top: "b20_1x7_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "b20_1x7_reduce_bn"
  type: "BatchNorm"
  bottom: "b20_1x7_reduce"
  top: "b20_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b20_1x7_reduce_scale"
  type: "Scale"
  bottom: "b20_1x7_reduce"
  top: "b20_1x7_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b20_1x7_reduce_relu"
  type: "ReLU"
  bottom: "b20_1x7_reduce"
  top: "b20_1x7_reduce"
}
layer {
  name: "b20_1x7"
  type: "Convolution"
  bottom: "b20_1x7_reduce"
  top: "b20_1x7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 160
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 3
    kernel_h: 1
    kernel_w: 7
  }
}
layer {
  name: "b20_1x7_bn"
  type: "BatchNorm"
  bottom: "b20_1x7"
  top: "b20_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b20_1x7_scale"
  type: "Scale"
  bottom: "b20_1x7"
  top: "b20_1x7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b20_1x7_relu"
  type: "ReLU"
  bottom: "b20_1x7"
  top: "b20_1x7"
}
layer {
  name: "b20_7x1"
  type: "Convolution"
  bottom: "b20_1x7"
  top: "b20_7x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 3
    pad_w: 0
    kernel_h: 7
    kernel_w: 1
  }
}
layer {
  name: "b20_7x1_bn"
  type: "BatchNorm"
  bottom: "b20_7x1"
  top: "b20_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "b20_7x1_scale"
  type: "Scale"
  bottom: "b20_7x1"
  top: "b20_7x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "b20_7x1_relu"
  type: "ReLU"
  bottom: "b20_7x1"
  top: "b20_7x1"
}
layer {
  name: "b20_concat"
  type: "Concat"
  bottom: "b20_1x1"
  bottom: "b20_7x1"
  top: "b20_concat"
}
layer {
  name: "b20_1x1_2"
  type: "Convolution"
  bottom: "b20_concat"
  top: "b20_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 1088
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "b20_1x1_2_power"
  type: "Power"
  bottom: "b20_1x1_2"
  top: "b20_1x1_2_power"
  power_param {
    power: 1
    scale: 0.1
    shift: 0
  }
}
layer {
  name: "b20_residual_eltwise"
  type: "Eltwise"
  bottom: "b19_residual_eltwise"
  bottom: "b20_1x1_2_power"
  top: "b20_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "b20_residual_eltwise_relu"
  type: "ReLU"
  bottom: "b20_residual_eltwise"
  top: "b20_residual_eltwise"
}
layer {
  name: "reduction_b_pool"
  type: "Pooling"
  bottom: "b20_residual_eltwise"
  top: "reduction_b_pool"
  pooling_param {
    pad: 1
    pool: MAX
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "reduction_b_3x3_reduce"
  type: "Convolution"
  bottom: "b20_residual_eltwise"
  top: "reduction_b_3x3_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "reduction_b_3x3_reduce_bn"
  type: "BatchNorm"
  bottom: "reduction_b_3x3_reduce"
  top: "reduction_b_3x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "reduction_b_3x3_reduce_scale"
  type: "Scale"
  bottom: "reduction_b_3x3_reduce"
  top: "reduction_b_3x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "reduction_b_3x3_reduce_relu"
  type: "ReLU"
  bottom: "reduction_b_3x3_reduce"
  top: "reduction_b_3x3_reduce"
}
layer {
  name: "reduction_b_3x3"
  type: "Convolution"
  bottom: "reduction_b_3x3_reduce"
  top: "reduction_b_3x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 384
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "reduction_b_3x3_bn"
  type: "BatchNorm"
  bottom: "reduction_b_3x3"
  top: "reduction_b_3x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "reduction_b_3x3_scale"
  type: "Scale"
  bottom: "reduction_b_3x3"
  top: "reduction_b_3x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "reduction_b_3x3_relu"
  type: "ReLU"
  bottom: "reduction_b_3x3"
  top: "reduction_b_3x3"
}
layer {
  name: "reduction_b_3x3_2_reduce"
  type: "Convolution"
  bottom: "b20_residual_eltwise"
  top: "reduction_b_3x3_2_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "reduction_b_3x3_2_reduce_bn"
  type: "BatchNorm"
  bottom: "reduction_b_3x3_2_reduce"
  top: "reduction_b_3x3_2_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "reduction_b_3x3_2_reduce_scale"
  type: "Scale"
  bottom: "reduction_b_3x3_2_reduce"
  top: "reduction_b_3x3_2_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "reduction_b_3x3_2_reduce_relu"
  type: "ReLU"
  bottom: "reduction_b_3x3_2_reduce"
  top: "reduction_b_3x3_2_reduce"
}
layer {
  name: "reduction_b_3x3_2"
  type: "Convolution"
  bottom: "reduction_b_3x3_2_reduce"
  top: "reduction_b_3x3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 288
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "reduction_b_3x3_2_bn"
  type: "BatchNorm"
  bottom: "reduction_b_3x3_2"
  top: "reduction_b_3x3_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "reduction_b_3x3_2_scale"
  type: "Scale"
  bottom: "reduction_b_3x3_2"
  top: "reduction_b_3x3_2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "reduction_b_3x3_2_relu"
  type: "ReLU"
  bottom: "reduction_b_3x3_2"
  top: "reduction_b_3x3_2"
}
layer {
  name: "reduction_b_3x3_3_reduce"
  type: "Convolution"
  bottom: "b20_residual_eltwise"
  top: "reduction_b_3x3_3_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "reduction_b_3x3_3_reduce_bn"
  type: "BatchNorm"
  bottom: "reduction_b_3x3_3_reduce"
  top: "reduction_b_3x3_3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "reduction_b_3x3_3_reduce_scale"
  type: "Scale"
  bottom: "reduction_b_3x3_3_reduce"
  top: "reduction_b_3x3_3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "reduction_b_3x3_3_reduce_relu"
  type: "ReLU"
  bottom: "reduction_b_3x3_3_reduce"
  top: "reduction_b_3x3_3_reduce"
}
layer {
  name: "reduction_b_3x3_3"
  type: "Convolution"
  bottom: "reduction_b_3x3_3_reduce"
  top: "reduction_b_3x3_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 288
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "reduction_b_3x3_3_bn"
  type: "BatchNorm"
  bottom: "reduction_b_3x3_3"
  top: "reduction_b_3x3_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "reduction_b_3x3_3_scale"
  type: "Scale"
  bottom: "reduction_b_3x3_3"
  top: "reduction_b_3x3_3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "reduction_b_3x3_3_relu"
  type: "ReLU"
  bottom: "reduction_b_3x3_3"
  top: "reduction_b_3x3_3"
}
layer {
  name: "reduction_b_3x3_4"
  type: "Convolution"
  bottom: "reduction_b_3x3_3"
  top: "reduction_b_3x3_4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 320
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "reduction_b_3x3_4_bn"
  type: "BatchNorm"
  bottom: "reduction_b_3x3_4"
  top: "reduction_b_3x3_4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "reduction_b_3x3_4_scale"
  type: "Scale"
  bottom: "reduction_b_3x3_4"
  top: "reduction_b_3x3_4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "reduction_b_3x3_4_relu"
  type: "ReLU"
  bottom: "reduction_b_3x3_4"
  top: "reduction_b_3x3_4"
}
layer {
  name: "reduction_b_concat"
  type: "Concat"
  bottom: "reduction_b_3x3"
  bottom: "reduction_b_3x3_2"
  bottom: "reduction_b_3x3_4"
  bottom: "reduction_b_pool"
  top: "reduction_b_concat"
}
layer {
  name: "c1_1x1"
  type: "Convolution"
  bottom: "reduction_b_concat"
  top: "c1_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "c1_1x1_bn"
  type: "BatchNorm"
  bottom: "c1_1x1"
  top: "c1_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c1_1x1_scale"
  type: "Scale"
  bottom: "c1_1x1"
  top: "c1_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c1_1x1_relu"
  type: "ReLU"
  bottom: "c1_1x1"
  top: "c1_1x1"
}
layer {
  name: "c1_1x3_reduce"
  type: "Convolution"
  bottom: "reduction_b_concat"
  top: "c1_1x3_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "c1_1x3_reduce_bn"
  type: "BatchNorm"
  bottom: "c1_1x3_reduce"
  top: "c1_1x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c1_1x3_reduce_scale"
  type: "Scale"
  bottom: "c1_1x3_reduce"
  top: "c1_1x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c1_1x3_reduce_relu"
  type: "ReLU"
  bottom: "c1_1x3_reduce"
  top: "c1_1x3_reduce"
}
layer {
  name: "c1_1x3"
  type: "Convolution"
  bottom: "c1_1x3_reduce"
  top: "c1_1x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 1
    kernel_h: 1
    kernel_w: 3
  }
}
layer {
  name: "c1_1x3_bn"
  type: "BatchNorm"
  bottom: "c1_1x3"
  top: "c1_1x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c1_1x3_scale"
  type: "Scale"
  bottom: "c1_1x3"
  top: "c1_1x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c1_1x3_relu"
  type: "ReLU"
  bottom: "c1_1x3"
  top: "c1_1x3"
}
layer {
  name: "c1_3x1"
  type: "Convolution"
  bottom: "c1_1x3"
  top: "c1_3x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "c1_3x1_bn"
  type: "BatchNorm"
  bottom: "c1_3x1"
  top: "c1_3x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c1_3x1_scale"
  type: "Scale"
  bottom: "c1_3x1"
  top: "c1_3x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c1_3x1_relu"
  type: "ReLU"
  bottom: "c1_3x1"
  top: "c1_3x1"
}
layer {
  name: "c1_concat"
  type: "Concat"
  bottom: "c1_1x1"
  bottom: "c1_3x1"
  top: "c1_concat"
}
layer {
  name: "c1_1x1_2"
  type: "Convolution"
  bottom: "c1_concat"
  top: "c1_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 2080
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "c1_1x1_2_power"
  type: "Power"
  bottom: "c1_1x1_2"
  top: "c1_1x1_2_power"
  power_param {
    power: 1
    scale: 0.2
    shift: 0
  }
}
layer {
  name: "c1_residual_eltwise"
  type: "Eltwise"
  bottom: "reduction_b_concat"
  bottom: "c1_1x1_2_power"
  top: "c1_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "c1_residual_eltwise_relu"
  type: "ReLU"
  bottom: "c1_residual_eltwise"
  top: "c1_residual_eltwise"
}
layer {
  name: "c2_1x1"
  type: "Convolution"
  bottom: "c1_residual_eltwise"
  top: "c2_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "c2_1x1_bn"
  type: "BatchNorm"
  bottom: "c2_1x1"
  top: "c2_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c2_1x1_scale"
  type: "Scale"
  bottom: "c2_1x1"
  top: "c2_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c2_1x1_relu"
  type: "ReLU"
  bottom: "c2_1x1"
  top: "c2_1x1"
}
layer {
  name: "c2_1x3_reduce"
  type: "Convolution"
  bottom: "c1_residual_eltwise"
  top: "c2_1x3_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "c2_1x3_reduce_bn"
  type: "BatchNorm"
  bottom: "c2_1x3_reduce"
  top: "c2_1x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c2_1x3_reduce_scale"
  type: "Scale"
  bottom: "c2_1x3_reduce"
  top: "c2_1x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c2_1x3_reduce_relu"
  type: "ReLU"
  bottom: "c2_1x3_reduce"
  top: "c2_1x3_reduce"
}
layer {
  name: "c2_1x3"
  type: "Convolution"
  bottom: "c2_1x3_reduce"
  top: "c2_1x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 1
    kernel_h: 1
    kernel_w: 3
  }
}
layer {
  name: "c2_1x3_bn"
  type: "BatchNorm"
  bottom: "c2_1x3"
  top: "c2_1x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c2_1x3_scale"
  type: "Scale"
  bottom: "c2_1x3"
  top: "c2_1x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c2_1x3_relu"
  type: "ReLU"
  bottom: "c2_1x3"
  top: "c2_1x3"
}
layer {
  name: "c2_3x1"
  type: "Convolution"
  bottom: "c2_1x3"
  top: "c2_3x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "c2_3x1_bn"
  type: "BatchNorm"
  bottom: "c2_3x1"
  top: "c2_3x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c2_3x1_scale"
  type: "Scale"
  bottom: "c2_3x1"
  top: "c2_3x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c2_3x1_relu"
  type: "ReLU"
  bottom: "c2_3x1"
  top: "c2_3x1"
}
layer {
  name: "c2_concat"
  type: "Concat"
  bottom: "c2_1x1"
  bottom: "c2_3x1"
  top: "c2_concat"
}
layer {
  name: "c2_1x1_2"
  type: "Convolution"
  bottom: "c2_concat"
  top: "c2_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 2080
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "c2_1x1_2_power"
  type: "Power"
  bottom: "c2_1x1_2"
  top: "c2_1x1_2_power"
  power_param {
    power: 1
    scale: 0.2
    shift: 0
  }
}
layer {
  name: "c2_residual_eltwise"
  type: "Eltwise"
  bottom: "c1_residual_eltwise"
  bottom: "c2_1x1_2_power"
  top: "c2_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "c2_residual_eltwise_relu"
  type: "ReLU"
  bottom: "c2_residual_eltwise"
  top: "c2_residual_eltwise"
}
layer {
  name: "c3_1x1"
  type: "Convolution"
  bottom: "c2_residual_eltwise"
  top: "c3_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "c3_1x1_bn"
  type: "BatchNorm"
  bottom: "c3_1x1"
  top: "c3_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c3_1x1_scale"
  type: "Scale"
  bottom: "c3_1x1"
  top: "c3_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c3_1x1_relu"
  type: "ReLU"
  bottom: "c3_1x1"
  top: "c3_1x1"
}
layer {
  name: "c3_1x3_reduce"
  type: "Convolution"
  bottom: "c2_residual_eltwise"
  top: "c3_1x3_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "c3_1x3_reduce_bn"
  type: "BatchNorm"
  bottom: "c3_1x3_reduce"
  top: "c3_1x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c3_1x3_reduce_scale"
  type: "Scale"
  bottom: "c3_1x3_reduce"
  top: "c3_1x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c3_1x3_reduce_relu"
  type: "ReLU"
  bottom: "c3_1x3_reduce"
  top: "c3_1x3_reduce"
}
layer {
  name: "c3_1x3"
  type: "Convolution"
  bottom: "c3_1x3_reduce"
  top: "c3_1x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 1
    kernel_h: 1
    kernel_w: 3
  }
}
layer {
  name: "c3_1x3_bn"
  type: "BatchNorm"
  bottom: "c3_1x3"
  top: "c3_1x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c3_1x3_scale"
  type: "Scale"
  bottom: "c3_1x3"
  top: "c3_1x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c3_1x3_relu"
  type: "ReLU"
  bottom: "c3_1x3"
  top: "c3_1x3"
}
layer {
  name: "c3_3x1"
  type: "Convolution"
  bottom: "c3_1x3"
  top: "c3_3x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "c3_3x1_bn"
  type: "BatchNorm"
  bottom: "c3_3x1"
  top: "c3_3x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c3_3x1_scale"
  type: "Scale"
  bottom: "c3_3x1"
  top: "c3_3x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c3_3x1_relu"
  type: "ReLU"
  bottom: "c3_3x1"
  top: "c3_3x1"
}
layer {
  name: "c3_concat"
  type: "Concat"
  bottom: "c3_1x1"
  bottom: "c3_3x1"
  top: "c3_concat"
}
layer {
  name: "c3_1x1_2"
  type: "Convolution"
  bottom: "c3_concat"
  top: "c3_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 2080
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "c3_1x1_2_power"
  type: "Power"
  bottom: "c3_1x1_2"
  top: "c3_1x1_2_power"
  power_param {
    power: 1
    scale: 0.2
    shift: 0
  }
}
layer {
  name: "c3_residual_eltwise"
  type: "Eltwise"
  bottom: "c2_residual_eltwise"
  bottom: "c3_1x1_2_power"
  top: "c3_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "c3_residual_eltwise_relu"
  type: "ReLU"
  bottom: "c3_residual_eltwise"
  top: "c3_residual_eltwise"
}
layer {
  name: "c4_1x1"
  type: "Convolution"
  bottom: "c3_residual_eltwise"
  top: "c4_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "c4_1x1_bn"
  type: "BatchNorm"
  bottom: "c4_1x1"
  top: "c4_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c4_1x1_scale"
  type: "Scale"
  bottom: "c4_1x1"
  top: "c4_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c4_1x1_relu"
  type: "ReLU"
  bottom: "c4_1x1"
  top: "c4_1x1"
}
layer {
  name: "c4_1x3_reduce"
  type: "Convolution"
  bottom: "c3_residual_eltwise"
  top: "c4_1x3_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "c4_1x3_reduce_bn"
  type: "BatchNorm"
  bottom: "c4_1x3_reduce"
  top: "c4_1x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c4_1x3_reduce_scale"
  type: "Scale"
  bottom: "c4_1x3_reduce"
  top: "c4_1x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c4_1x3_reduce_relu"
  type: "ReLU"
  bottom: "c4_1x3_reduce"
  top: "c4_1x3_reduce"
}
layer {
  name: "c4_1x3"
  type: "Convolution"
  bottom: "c4_1x3_reduce"
  top: "c4_1x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 1
    kernel_h: 1
    kernel_w: 3
  }
}
layer {
  name: "c4_1x3_bn"
  type: "BatchNorm"
  bottom: "c4_1x3"
  top: "c4_1x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c4_1x3_scale"
  type: "Scale"
  bottom: "c4_1x3"
  top: "c4_1x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c4_1x3_relu"
  type: "ReLU"
  bottom: "c4_1x3"
  top: "c4_1x3"
}
layer {
  name: "c4_3x1"
  type: "Convolution"
  bottom: "c4_1x3"
  top: "c4_3x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "c4_3x1_bn"
  type: "BatchNorm"
  bottom: "c4_3x1"
  top: "c4_3x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c4_3x1_scale"
  type: "Scale"
  bottom: "c4_3x1"
  top: "c4_3x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c4_3x1_relu"
  type: "ReLU"
  bottom: "c4_3x1"
  top: "c4_3x1"
}
layer {
  name: "c4_concat"
  type: "Concat"
  bottom: "c4_1x1"
  bottom: "c4_3x1"
  top: "c4_concat"
}
layer {
  name: "c4_1x1_2"
  type: "Convolution"
  bottom: "c4_concat"
  top: "c4_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 2080
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "c4_1x1_2_power"
  type: "Power"
  bottom: "c4_1x1_2"
  top: "c4_1x1_2_power"
  power_param {
    power: 1
    scale: 0.2
    shift: 0
  }
}
layer {
  name: "c4_residual_eltwise"
  type: "Eltwise"
  bottom: "c3_residual_eltwise"
  bottom: "c4_1x1_2_power"
  top: "c4_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "c4_residual_eltwise_relu"
  type: "ReLU"
  bottom: "c4_residual_eltwise"
  top: "c4_residual_eltwise"
}
layer {
  name: "c5_1x1"
  type: "Convolution"
  bottom: "c4_residual_eltwise"
  top: "c5_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "c5_1x1_bn"
  type: "BatchNorm"
  bottom: "c5_1x1"
  top: "c5_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c5_1x1_scale"
  type: "Scale"
  bottom: "c5_1x1"
  top: "c5_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c5_1x1_relu"
  type: "ReLU"
  bottom: "c5_1x1"
  top: "c5_1x1"
}
layer {
  name: "c5_1x3_reduce"
  type: "Convolution"
  bottom: "c4_residual_eltwise"
  top: "c5_1x3_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "c5_1x3_reduce_bn"
  type: "BatchNorm"
  bottom: "c5_1x3_reduce"
  top: "c5_1x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c5_1x3_reduce_scale"
  type: "Scale"
  bottom: "c5_1x3_reduce"
  top: "c5_1x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c5_1x3_reduce_relu"
  type: "ReLU"
  bottom: "c5_1x3_reduce"
  top: "c5_1x3_reduce"
}
layer {
  name: "c5_1x3"
  type: "Convolution"
  bottom: "c5_1x3_reduce"
  top: "c5_1x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 1
    kernel_h: 1
    kernel_w: 3
  }
}
layer {
  name: "c5_1x3_bn"
  type: "BatchNorm"
  bottom: "c5_1x3"
  top: "c5_1x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c5_1x3_scale"
  type: "Scale"
  bottom: "c5_1x3"
  top: "c5_1x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c5_1x3_relu"
  type: "ReLU"
  bottom: "c5_1x3"
  top: "c5_1x3"
}
layer {
  name: "c5_3x1"
  type: "Convolution"
  bottom: "c5_1x3"
  top: "c5_3x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "c5_3x1_bn"
  type: "BatchNorm"
  bottom: "c5_3x1"
  top: "c5_3x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c5_3x1_scale"
  type: "Scale"
  bottom: "c5_3x1"
  top: "c5_3x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c5_3x1_relu"
  type: "ReLU"
  bottom: "c5_3x1"
  top: "c5_3x1"
}
layer {
  name: "c5_concat"
  type: "Concat"
  bottom: "c5_1x1"
  bottom: "c5_3x1"
  top: "c5_concat"
}
layer {
  name: "c5_1x1_2"
  type: "Convolution"
  bottom: "c5_concat"
  top: "c5_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 2080
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "c5_1x1_2_power"
  type: "Power"
  bottom: "c5_1x1_2"
  top: "c5_1x1_2_power"
  power_param {
    power: 1
    scale: 0.2
    shift: 0
  }
}
layer {
  name: "c5_residual_eltwise"
  type: "Eltwise"
  bottom: "c4_residual_eltwise"
  bottom: "c5_1x1_2_power"
  top: "c5_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "c5_residual_eltwise_relu"
  type: "ReLU"
  bottom: "c5_residual_eltwise"
  top: "c5_residual_eltwise"
}
layer {
  name: "c6_1x1"
  type: "Convolution"
  bottom: "c5_residual_eltwise"
  top: "c6_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "c6_1x1_bn"
  type: "BatchNorm"
  bottom: "c6_1x1"
  top: "c6_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c6_1x1_scale"
  type: "Scale"
  bottom: "c6_1x1"
  top: "c6_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c6_1x1_relu"
  type: "ReLU"
  bottom: "c6_1x1"
  top: "c6_1x1"
}
layer {
  name: "c6_1x3_reduce"
  type: "Convolution"
  bottom: "c5_residual_eltwise"
  top: "c6_1x3_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "c6_1x3_reduce_bn"
  type: "BatchNorm"
  bottom: "c6_1x3_reduce"
  top: "c6_1x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c6_1x3_reduce_scale"
  type: "Scale"
  bottom: "c6_1x3_reduce"
  top: "c6_1x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c6_1x3_reduce_relu"
  type: "ReLU"
  bottom: "c6_1x3_reduce"
  top: "c6_1x3_reduce"
}
layer {
  name: "c6_1x3"
  type: "Convolution"
  bottom: "c6_1x3_reduce"
  top: "c6_1x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 1
    kernel_h: 1
    kernel_w: 3
  }
}
layer {
  name: "c6_1x3_bn"
  type: "BatchNorm"
  bottom: "c6_1x3"
  top: "c6_1x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c6_1x3_scale"
  type: "Scale"
  bottom: "c6_1x3"
  top: "c6_1x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c6_1x3_relu"
  type: "ReLU"
  bottom: "c6_1x3"
  top: "c6_1x3"
}
layer {
  name: "c6_3x1"
  type: "Convolution"
  bottom: "c6_1x3"
  top: "c6_3x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "c6_3x1_bn"
  type: "BatchNorm"
  bottom: "c6_3x1"
  top: "c6_3x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c6_3x1_scale"
  type: "Scale"
  bottom: "c6_3x1"
  top: "c6_3x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c6_3x1_relu"
  type: "ReLU"
  bottom: "c6_3x1"
  top: "c6_3x1"
}
layer {
  name: "c6_concat"
  type: "Concat"
  bottom: "c6_1x1"
  bottom: "c6_3x1"
  top: "c6_concat"
}
layer {
  name: "c6_1x1_2"
  type: "Convolution"
  bottom: "c6_concat"
  top: "c6_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 2080
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "c6_1x1_2_power"
  type: "Power"
  bottom: "c6_1x1_2"
  top: "c6_1x1_2_power"
  power_param {
    power: 1
    scale: 0.2
    shift: 0
  }
}
layer {
  name: "c6_residual_eltwise"
  type: "Eltwise"
  bottom: "c5_residual_eltwise"
  bottom: "c6_1x1_2_power"
  top: "c6_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "c6_residual_eltwise_relu"
  type: "ReLU"
  bottom: "c6_residual_eltwise"
  top: "c6_residual_eltwise"
}
layer {
  name: "c7_1x1"
  type: "Convolution"
  bottom: "c6_residual_eltwise"
  top: "c7_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "c7_1x1_bn"
  type: "BatchNorm"
  bottom: "c7_1x1"
  top: "c7_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c7_1x1_scale"
  type: "Scale"
  bottom: "c7_1x1"
  top: "c7_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c7_1x1_relu"
  type: "ReLU"
  bottom: "c7_1x1"
  top: "c7_1x1"
}
layer {
  name: "c7_1x3_reduce"
  type: "Convolution"
  bottom: "c6_residual_eltwise"
  top: "c7_1x3_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "c7_1x3_reduce_bn"
  type: "BatchNorm"
  bottom: "c7_1x3_reduce"
  top: "c7_1x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c7_1x3_reduce_scale"
  type: "Scale"
  bottom: "c7_1x3_reduce"
  top: "c7_1x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c7_1x3_reduce_relu"
  type: "ReLU"
  bottom: "c7_1x3_reduce"
  top: "c7_1x3_reduce"
}
layer {
  name: "c7_1x3"
  type: "Convolution"
  bottom: "c7_1x3_reduce"
  top: "c7_1x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 1
    kernel_h: 1
    kernel_w: 3
  }
}
layer {
  name: "c7_1x3_bn"
  type: "BatchNorm"
  bottom: "c7_1x3"
  top: "c7_1x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c7_1x3_scale"
  type: "Scale"
  bottom: "c7_1x3"
  top: "c7_1x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c7_1x3_relu"
  type: "ReLU"
  bottom: "c7_1x3"
  top: "c7_1x3"
}
layer {
  name: "c7_3x1"
  type: "Convolution"
  bottom: "c7_1x3"
  top: "c7_3x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "c7_3x1_bn"
  type: "BatchNorm"
  bottom: "c7_3x1"
  top: "c7_3x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c7_3x1_scale"
  type: "Scale"
  bottom: "c7_3x1"
  top: "c7_3x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c7_3x1_relu"
  type: "ReLU"
  bottom: "c7_3x1"
  top: "c7_3x1"
}
layer {
  name: "c7_concat"
  type: "Concat"
  bottom: "c7_1x1"
  bottom: "c7_3x1"
  top: "c7_concat"
}
layer {
  name: "c7_1x1_2"
  type: "Convolution"
  bottom: "c7_concat"
  top: "c7_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 2080
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "c7_1x1_2_power"
  type: "Power"
  bottom: "c7_1x1_2"
  top: "c7_1x1_2_power"
  power_param {
    power: 1
    scale: 0.2
    shift: 0
  }
}
layer {
  name: "c7_residual_eltwise"
  type: "Eltwise"
  bottom: "c6_residual_eltwise"
  bottom: "c7_1x1_2_power"
  top: "c7_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "c7_residual_eltwise_relu"
  type: "ReLU"
  bottom: "c7_residual_eltwise"
  top: "c7_residual_eltwise"
}
layer {
  name: "c8_1x1"
  type: "Convolution"
  bottom: "c7_residual_eltwise"
  top: "c8_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "c8_1x1_bn"
  type: "BatchNorm"
  bottom: "c8_1x1"
  top: "c8_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c8_1x1_scale"
  type: "Scale"
  bottom: "c8_1x1"
  top: "c8_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c8_1x1_relu"
  type: "ReLU"
  bottom: "c8_1x1"
  top: "c8_1x1"
}
layer {
  name: "c8_1x3_reduce"
  type: "Convolution"
  bottom: "c7_residual_eltwise"
  top: "c8_1x3_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "c8_1x3_reduce_bn"
  type: "BatchNorm"
  bottom: "c8_1x3_reduce"
  top: "c8_1x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c8_1x3_reduce_scale"
  type: "Scale"
  bottom: "c8_1x3_reduce"
  top: "c8_1x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c8_1x3_reduce_relu"
  type: "ReLU"
  bottom: "c8_1x3_reduce"
  top: "c8_1x3_reduce"
}
layer {
  name: "c8_1x3"
  type: "Convolution"
  bottom: "c8_1x3_reduce"
  top: "c8_1x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 1
    kernel_h: 1
    kernel_w: 3
  }
}
layer {
  name: "c8_1x3_bn"
  type: "BatchNorm"
  bottom: "c8_1x3"
  top: "c8_1x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c8_1x3_scale"
  type: "Scale"
  bottom: "c8_1x3"
  top: "c8_1x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c8_1x3_relu"
  type: "ReLU"
  bottom: "c8_1x3"
  top: "c8_1x3"
}
layer {
  name: "c8_3x1"
  type: "Convolution"
  bottom: "c8_1x3"
  top: "c8_3x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "c8_3x1_bn"
  type: "BatchNorm"
  bottom: "c8_3x1"
  top: "c8_3x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c8_3x1_scale"
  type: "Scale"
  bottom: "c8_3x1"
  top: "c8_3x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c8_3x1_relu"
  type: "ReLU"
  bottom: "c8_3x1"
  top: "c8_3x1"
}
layer {
  name: "c8_concat"
  type: "Concat"
  bottom: "c8_1x1"
  bottom: "c8_3x1"
  top: "c8_concat"
}
layer {
  name: "c8_1x1_2"
  type: "Convolution"
  bottom: "c8_concat"
  top: "c8_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 2080
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "c8_1x1_2_power"
  type: "Power"
  bottom: "c8_1x1_2"
  top: "c8_1x1_2_power"
  power_param {
    power: 1
    scale: 0.2
    shift: 0
  }
}
layer {
  name: "c8_residual_eltwise"
  type: "Eltwise"
  bottom: "c7_residual_eltwise"
  bottom: "c8_1x1_2_power"
  top: "c8_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "c8_residual_eltwise_relu"
  type: "ReLU"
  bottom: "c8_residual_eltwise"
  top: "c8_residual_eltwise"
}
layer {
  name: "c9_1x1"
  type: "Convolution"
  bottom: "c8_residual_eltwise"
  top: "c9_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "c9_1x1_bn"
  type: "BatchNorm"
  bottom: "c9_1x1"
  top: "c9_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c9_1x1_scale"
  type: "Scale"
  bottom: "c9_1x1"
  top: "c9_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c9_1x1_relu"
  type: "ReLU"
  bottom: "c9_1x1"
  top: "c9_1x1"
}
layer {
  name: "c9_1x3_reduce"
  type: "Convolution"
  bottom: "c8_residual_eltwise"
  top: "c9_1x3_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "c9_1x3_reduce_bn"
  type: "BatchNorm"
  bottom: "c9_1x3_reduce"
  top: "c9_1x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c9_1x3_reduce_scale"
  type: "Scale"
  bottom: "c9_1x3_reduce"
  top: "c9_1x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c9_1x3_reduce_relu"
  type: "ReLU"
  bottom: "c9_1x3_reduce"
  top: "c9_1x3_reduce"
}
layer {
  name: "c9_1x3"
  type: "Convolution"
  bottom: "c9_1x3_reduce"
  top: "c9_1x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 1
    kernel_h: 1
    kernel_w: 3
  }
}
layer {
  name: "c9_1x3_bn"
  type: "BatchNorm"
  bottom: "c9_1x3"
  top: "c9_1x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c9_1x3_scale"
  type: "Scale"
  bottom: "c9_1x3"
  top: "c9_1x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c9_1x3_relu"
  type: "ReLU"
  bottom: "c9_1x3"
  top: "c9_1x3"
}
layer {
  name: "c9_3x1"
  type: "Convolution"
  bottom: "c9_1x3"
  top: "c9_3x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "c9_3x1_bn"
  type: "BatchNorm"
  bottom: "c9_3x1"
  top: "c9_3x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c9_3x1_scale"
  type: "Scale"
  bottom: "c9_3x1"
  top: "c9_3x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c9_3x1_relu"
  type: "ReLU"
  bottom: "c9_3x1"
  top: "c9_3x1"
}
layer {
  name: "c9_concat"
  type: "Concat"
  bottom: "c9_1x1"
  bottom: "c9_3x1"
  top: "c9_concat"
}
layer {
  name: "c9_1x1_2"
  type: "Convolution"
  bottom: "c9_concat"
  top: "c9_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 2080
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "c9_1x1_2_power"
  type: "Power"
  bottom: "c9_1x1_2"
  top: "c9_1x1_2_power"
  power_param {
    power: 1
    scale: 0.2
    shift: 0
  }
}
layer {
  name: "c9_residual_eltwise"
  type: "Eltwise"
  bottom: "c8_residual_eltwise"
  bottom: "c9_1x1_2_power"
  top: "c9_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "c9_residual_eltwise_relu"
  type: "ReLU"
  bottom: "c9_residual_eltwise"
  top: "c9_residual_eltwise"
}
layer {
  name: "c10_1x1"
  type: "Convolution"
  bottom: "c9_residual_eltwise"
  top: "c10_1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "c10_1x1_bn"
  type: "BatchNorm"
  bottom: "c10_1x1"
  top: "c10_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c10_1x1_scale"
  type: "Scale"
  bottom: "c10_1x1"
  top: "c10_1x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c10_1x1_relu"
  type: "ReLU"
  bottom: "c10_1x1"
  top: "c10_1x1"
}
layer {
  name: "c10_1x3_reduce"
  type: "Convolution"
  bottom: "c9_residual_eltwise"
  top: "c10_1x3_reduce"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 192
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "c10_1x3_reduce_bn"
  type: "BatchNorm"
  bottom: "c10_1x3_reduce"
  top: "c10_1x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c10_1x3_reduce_scale"
  type: "Scale"
  bottom: "c10_1x3_reduce"
  top: "c10_1x3_reduce"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c10_1x3_reduce_relu"
  type: "ReLU"
  bottom: "c10_1x3_reduce"
  top: "c10_1x3_reduce"
}
layer {
  name: "c10_1x3"
  type: "Convolution"
  bottom: "c10_1x3_reduce"
  top: "c10_1x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 224
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 0
    pad_w: 1
    kernel_h: 1
    kernel_w: 3
  }
}
layer {
  name: "c10_1x3_bn"
  type: "BatchNorm"
  bottom: "c10_1x3"
  top: "c10_1x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c10_1x3_scale"
  type: "Scale"
  bottom: "c10_1x3"
  top: "c10_1x3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c10_1x3_relu"
  type: "ReLU"
  bottom: "c10_1x3"
  top: "c10_1x3"
}
layer {
  name: "c10_3x1"
  type: "Convolution"
  bottom: "c10_1x3"
  top: "c10_3x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 256
    bias_term: false
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    pad_h: 1
    pad_w: 0
    kernel_h: 3
    kernel_w: 1
  }
}
layer {
  name: "c10_3x1_bn"
  type: "BatchNorm"
  bottom: "c10_3x1"
  top: "c10_3x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "c10_3x1_scale"
  type: "Scale"
  bottom: "c10_3x1"
  top: "c10_3x1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "c10_3x1_relu"
  type: "ReLU"
  bottom: "c10_3x1"
  top: "c10_3x1"
}
layer {
  name: "c10_concat"
  type: "Concat"
  bottom: "c10_1x1"
  bottom: "c10_3x1"
  top: "c10_concat"
}
layer {
  name: "c10_1x1_2"
  type: "Convolution"
  bottom: "c10_concat"
  top: "c10_1x1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 2080
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "c10_1x1_2_power"
  type: "Power"
  bottom: "c10_1x1_2"
  top: "c10_1x1_2_power"
  power_param {
    power: 1
    scale: 0.2
    shift: 0
  }
}
layer {
  name: "c10_residual_eltwise"
  type: "Eltwise"
  bottom: "c9_residual_eltwise"
  bottom: "c10_1x1_2_power"
  top: "c10_residual_eltwise"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv7b_1x1_s1"
  type: "Convolution"
  bottom: "c10_residual_eltwise"
  top: "conv7b_1x1_s1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  convolution_param {
    num_output: 1536
    bias_term: false
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.01
    }
  }
}
layer {
  name: "conv7b_1x1_s1_bn"
  type: "BatchNorm"
  bottom: "conv7b_1x1_s1"
  top: "conv7b_1x1_s1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv7b_1x1_s1_scale"
  type: "Scale"
  bottom: "conv7b_1x1_s1"
  top: "conv7b_1x1_s1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv7b_1x1_s1_relu"
  type: "ReLU"
  bottom: "conv7b_1x1_s1"
  top: "conv7b_1x1_s1"
}
#========= RPN ============
layer {
  name: "rpn_conv/3x3"
  type: "Convolution"
  bottom: "b20_residual_eltwise"
  top: "rpn/output"
  param { lr_mult: 1.0 }
  param { lr_mult: 2.0 }
  convolution_param {
    num_output: 1024
    kernel_size: 3 pad: 1 stride: 1
    weight_filler { type: "gaussian" std: 0.01 }
    bias_filler { type: "constant" value: 0 }
  }
}

layer {
  name: "rpn_relu/3x3"
  type: "ReLU"
  bottom: "rpn/output"
  top: "rpn/output"
}

layer {
  name: "rpn_cls_score"
  type: "Convolution"
  bottom: "rpn/output"
  top: "rpn_cls_score"
  param { lr_mult: 1.0 }
  param { lr_mult: 2.0 }
  convolution_param {
    num_output: 40   # 2(bg/fg) * 9(anchors)
      kernel_size: 1 pad: 0 stride: 1
    weight_filler { type: "gaussian" std: 0.01 }
    bias_filler { type: "constant" value: 0 }
  }
}

layer {
  name: "rpn_bbox_pred"
  type: "Convolution"
  bottom: "rpn/output"
  top: "rpn_bbox_pred"
  param { lr_mult: 1.0 }
  param { lr_mult: 2.0 }
  convolution_param {
    num_output: 80   # 4 * 9(anchors)
    kernel_size: 1 pad: 0 stride: 1
    weight_filler { type: "gaussian" std: 0.01 }
    bias_filler { type: "constant" value: 0 }
  }
}

layer {
  bottom: "rpn_cls_score"
  top: "rpn_cls_score_reshape"
  name: "rpn_cls_score_reshape"
  type: "Reshape"
  reshape_param { shape { dim: 0 dim: 2 dim: -1 dim: 0 } }
}
##=========rpn end==================


#========= RoI Proposal ============

layer {
  name: "rpn_cls_prob"
  type: "Softmax"
  bottom: "rpn_cls_score_reshape"
  top: "rpn_cls_prob"
}

layer {
  name: 'rpn_cls_prob_reshape'
  type: 'Reshape'
  bottom: 'rpn_cls_prob'
  top: 'rpn_cls_prob_reshape'
  reshape_param { shape { dim: 0 dim: 40 dim: -1 dim: 0 } }
}

layer {
  name: 'proposal'
  type: 'Python'
  bottom: 'rpn_cls_prob_reshape'
  bottom: 'rpn_bbox_pred'
  bottom: 'im_info'
  top: 'rois'
  #top: 'rpn_scores'
  python_param {
    module: 'rpn.proposal_layer'
    layer: 'ProposalLayer'
    param_str: "'feat_stride': 16"
  }
}

#========= RCNN ============

layer {
  name: "roi_pool5"
  type: "ROIPooling"
  bottom: "conv7b_1x1_s1"
  bottom: "rois"
  top: "roi_pool5"
  roi_pooling_param {
    pooled_w: 8    # has stride 2 as in vgg16 in this step.
    pooled_h: 8
    spatial_scale: 0.0625 # 1/16
  }
}
layer {
  name: "pool5_1st"
  type: "Pooling"
  bottom: "roi_pool5"
  top: "pool5_1st"
  pooling_param {
    pool: AVE
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "pool5_1st"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1024
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc1_relu"
  type: "ReLU"
  bottom: "fc1"
  top: "fc1"
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "fc1"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1024
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc2_relu"
  type: "ReLU"
  bottom: "fc2"
  top: "fc2"
}


layer {
  name: "cls_score_det"
  type: "InnerProduct"
  bottom: "fc2"
  top: "cls_score_det"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 501
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bbox_pred_det"
  type: "InnerProduct"
  bottom: "fc2"
  top: "bbox_pred_det"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2004
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "proposals_2nd"
  type: "DecodeBBox"
  bottom: "bbox_pred_det"
  bottom: "rois"
  top: "proposals_2nd"
  bbox_reg_param {
    bbox_mean: 0 bbox_mean: 0 bbox_mean: 0 bbox_mean: 0
    bbox_std: 0.1 bbox_std: 0.1 bbox_std: 0.2 bbox_std: 0.2
  }
  propagate_down: 0
  propagate_down: 0
}

#========= Stage2_RCNN ============
layer {
  name: "roi_pool5_2nd"
  type: "ROIPooling"
  bottom: "conv7b_1x1_s1"
  bottom: "proposals_2nd"
  top: "roi_pool5_2nd"
  roi_pooling_param {
    pooled_w: 8
    pooled_h: 8
    spatial_scale: 0.0625 # 1/16
  }
}
layer {
  name: "pool5_2nd"
  type: "Pooling"
  bottom: "roi_pool5_2nd"
  top: "pool5_2nd"
  pooling_param {
    pool: AVE
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc1_2nd"
  type: "InnerProduct"
  bottom: "pool5_2nd"
  top: "fc1_2nd"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1024
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc1_2nd_relu"
  type: "ReLU"
  bottom: "fc1_2nd"
  top: "fc1_2nd"
}
layer {
  name: "fc2_2nd"
  type: "InnerProduct"
  bottom: "fc1_2nd"
  top: "fc2_2nd"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1024
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc2_2nd_relu"
  type: "ReLU"
  bottom: "fc2_2nd"
  top: "fc2_2nd"
}
layer {
  name: "cls_score_det_2nd"
  type: "InnerProduct"
  bottom: "fc2_2nd"
  top: "cls_score_det_2nd"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 501
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bbox_pred_det_2nd"
  type: "InnerProduct"
  bottom: "fc2_2nd"
  top: "bbox_pred_det_2nd"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2004
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "proposals_3rd"
  type: "DecodeBBox"
  bottom: "bbox_pred_det_2nd"
  bottom: "proposals_2nd"
  top: "proposals_3rd"
  bbox_reg_param {
    bbox_mean: 0 bbox_mean: 0 bbox_mean: 0 bbox_mean: 0
    bbox_std: 0.1 bbox_std: 0.1 bbox_std: 0.2 bbox_std: 0.2
  }
  propagate_down: 0
  propagate_down: 0
}
layer {
  name: "roi_pool5_3rd"
  type: "ROIPooling"
  bottom: "conv7b_1x1_s1"
  bottom: "proposals_3rd"
  top: "roi_pool5_3rd"
  roi_pooling_param {
    pooled_w: 8
    pooled_h: 8
    spatial_scale: 0.0625 # 1/16
  }
}
layer {
  name: "pool5_3rd"
  type: "Pooling"
  bottom: "roi_pool5_3rd"
  top: "pool5_3rd"
  pooling_param {
    pool: AVE
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "fc1_3rd"
  type: "InnerProduct"
  bottom: "pool5_3rd"
  top: "fc1_3rd"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1024
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc1_3rd_relu"
  type: "ReLU"
  bottom: "fc1_3rd"
  top: "fc1_3rd"
}
layer {
  name: "fc2_3rd"
  type: "InnerProduct"
  bottom: "fc1_3rd"
  top: "fc2_3rd"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1024
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "fc2_3rd_relu"
  type: "ReLU"
  bottom: "fc2_3rd"
  top: "fc2_3rd"
}
layer {
  name: "cls_score_det_3rd"
  type: "InnerProduct"
  bottom: "fc2_3rd"
  top: "cls_score_det_3rd"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 501
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "bbox_pred_det_3rd"
  type: "InnerProduct"
  bottom: "fc2_3rd"
  top: "bbox_pred_det_3rd"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2004
    weight_filler {
      type: "gaussian"
      std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "cls_prob_det_3rd"
  type: "Softmax"
  bottom: "cls_score_det_3rd"
  top: "cls_prob_det_3rd"
  softmax_param {
    axis: 1
  }
}
layer {
  name: "cls_prob_det_2nd"
  type: "Softmax"
  bottom: "cls_score_det_2nd"
  top: "cls_prob_det_2nd"
  softmax_param {
    axis: 1
  }
}
layer {
  name: "cls_prob_det"
  type: "Softmax"
  bottom: "cls_score_det"
  top: "cls_prob_det"
  softmax_param {
    axis: 1
  }
}
layer {
  name: "cls_prob_2nd_to_3rd_avg"
  type: "Eltwise"
  bottom: "cls_prob_det_3rd"
  bottom: "cls_prob_det_2nd"
  top: "cls_prob_2nd_to_3rd_avg"
  eltwise_param {
    operation: SUM
    coeff: 0.5
    coeff: 0.5
  }
}

layer {
  name: "cls_prob_1st_to_3rd_avg"
  type: "Eltwise"
  bottom: "cls_prob_det_3rd"
  bottom: "cls_prob_det_2nd"
  bottom: "cls_prob_det"
  top: "cls_prob_1st_to_3rd_avg"
  eltwise_param {
    operation: SUM
    coeff: 0.333333333
    coeff: 0.333333333
    coeff: 0.333333333
  }
}

